
<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1" />

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <title>Numerical Methods - Curve Fitting 2</title>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

  <!-- General and theme style sheets -->
  <link rel="stylesheet" href="../../../revealjs/css/reveal.css">
  <link rel="stylesheet" href="../../../revealjs/css/theme/beige.css" id="theme">

  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
    if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = '../../../revealjs/css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    }

  </script>

  <!--[if lt IE 9]>
  <script src="../../../revealjs//lib/js/html5shiv.js"></script>
<![endif]-->

<!-- Loading the mathjax macro -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
  processEscapes: true,
  processEnvironments: true
},
// Center justify equations in code and markdown cells. Elsewhere
// we use CSS to left justify single line equations in code cells.
displayAlign: 'center',
"HTML-CSS": {
styles: {'.MathJax_Display': {"margin": 0}},
linebreaks: { automatic: true }
}
});
</script>
<!-- End of mathjax configuration -->

<!-- Get Font-awesome from cdn -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.css">

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="../custom.css">

</head>
<body>

  <div class="reveal">
    <div class="slides">

      <section data-background-image="https://upload.wikimedia.org/wikipedia/commons/2/20/Coloured_Voronoi_2D.svg">
        <h2 style="color:white;" id="Numerical Methods Week 3">Numerical Methods Week 3<a class="anchor-link" href="#Numerical Methods Week 3">&#182;</a></h2>
        <h1 style="color:white;" id="Curve Fitting 1">Curve Fitting 1<a class="anchor-link" href="#Curve Fitting 1">&#182;</a></h1>
        <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
        8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
        <p>We continue with Curve Fitting. This week polynomial and multiple linear regression.</p>
        <p>Reading: Capra and Canale, introduction to part 5 and chapter 17.</p>
      </div>
      <br>
      <div style="background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
      8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
      <p>Learning outcomes:</p>
      <li> Extend the work on Linear Regression to polynomial and multiple variables.</li>
      <li> Combine C++ and analytical or other platforms.</li>
      <li> Check your code works correctly, via an external reference.</li>	
    </div>
    <br>
    <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
    8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">

    <h3>Matt Watkins mwatkins@lincoln.ac.uk<a class="anchor-link" href="#Matt Watkins">&#182;</a></h3>
  </div>
</section>

<section>
  <section>
    <div class="inner_cell">    
      <h2 id="Least-squares Regression - summary">Least Squares Regression  - summary<a class="anchor-link" href="#Least Squares Regression - summary">&#182;</a></h2>
      <p> we saw in <a href='https://mattatlincoln.github.io/teaching/numerical_methods/lecture_1/#/'>lecture 1</a> that given our assumption of a straightline
       $$
       y_i = a_0 + a_1 x_i + e_i
       $$
       the error at each point is given by
       $$
       e_i  = y_i - a_0 - a_1 x_i
       $$
     </p>
     <p> We take the sum of the squares of the errors
       $$
       S_r = \sum_{i=0}^{n-1} e_i^2 = \sum_{i=0}^{n-1}(y_i - a_0 - a_1 x_i)^2
       $$
       as our error criterion.
     </p>
   </div>
 </section>
 <section>
  <div class="inner_cell">
   <p> We can find an optimal $a_1$ and $a_0$.
     $$
     a_1 = \frac{n \sum x_i y_i - \sum x_i \sum y_i}{n \sum x_i^2 - (\sum x_i)^2}
     $$
     and 
     $$
     a_0 = \frac{\sum y_i}{n} - a_1 \frac{\sum x_i}{n} = \bar{y} - a_1 \bar{x}
     $$
   </p>
   <p>$\bar{y}$ and $\bar{x}$ are the means of the $x$ and $y$ values.
    $$
    \bar{y} = \frac{1}{N}\sum_{i=0}^{N-1} y_i \qquad \bar{x} = \frac{1}{N}\sum_{i=0}^{N-1} x_i
    $$
  </p>
  <br>
  <p> Correlation of the data: covariance of the data $(x,y)$ divided by the standard deviation of $x$ and $y$.<br><br>
   $$
   r = \frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{\sqrt{n \sum x_i^2 - (\sum x_i)^2}\sqrt{n \sum y_i^2 - (\sum y_i)^2}}
   $$
 </p>      
</div>
</section>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Exercises">Exercises<a class="anchor-link" href="#Exercises">&#182;</a></h2>
    <p>Remember in week 1 we looked at finding sums:</p>
    <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
    8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
    <li> What is $\sum_{n=1}^{100} n$</li>  
    <li> What is $\sum_{n=2}^{200} 2n$</li> 
    <li> What is $\sum_{n=0}^{99} 2n^2$</li>
  </div>

  <p>Now add an extra array into your code and calculate the following:</p>

  <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
  <p>$\text{let } x_i = i, i = {1,2,\dots,10}$ and $\text{let } y_i = 2i + 0.3$ for $i = {1,2,\dots,10}$</p>
  <li> What is $\sum_i x_i$</li>  
  <li> What is $\sum_i y_i$</li>
  <li> What is $\sum_i x_i y_i$</li>
  <li> What are $\bar{y}$ and $\bar{x}$</li>
  <li> Find the parameters $a_0$ and $a_1$ for a linear regression model of this data.</li>
</div>
<p> Check your results are correct! I'd suggest using both inspection and Excel.</p>      
</div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Array bounds">Array bounds<a class="anchor-link" href="#Array bounds">&#182;</a></h2>
    <p> This code example goes wrong because it tries to access x[10]</p>
    <p> C arrays start from element [0] and go up to [n-1].</p>
    <p> So in the code below x[0], x[1], ... x[9] are assigned. </p>
    <p> But in the loop, x[10] just prints whatever happens to exist in that memory location. </p>
    <pre data-lang="c++">
     <code class='c++'>
      // Be careful of array bounds
      #include&ltiostream>

      int main()
      {
        // assign an array of length 10
        double x[10] = {0,1,2,3,4,5,6,7,8,9};

        // loop over array elements, be very careful!
        // array elements not assigned will contain random data
        // visual studio may complain (depends on version, it seems)
        for (int i = 0; i < 11; i++) {
        std::cout << "i = " << i << ", x[" << i << "] = " << x[i] << "\n";
      }
    }	  
  </code>
</pre>
</div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Polynomial Regression">Polynomial Least-squares Regression<a class="anchor-link" href="#Polynomial Regression">&#182;</a></h2>
    <p>We can easily extend our method to deal with polynomicals:
     $$
     y_i = a_0 + a_1 x_i + a_2 x_i^2 + \ldots + a_n x_i^n  + e_i
     $$
   </p>
   <p>and an overall error function
    $$
    S_r = \sum_{i=0}^{N-1} (y_i - a_0 - a_1 x_i - a_2 x_i^2 - \ldots - a_n x_i^n)^2
    $$
  </p>
  <p>We then take partial derivatives with respect to the parameters ($a_0 \ldots a_n$) to get a set of equations.</p>
  <p>Setting these equations equal to zero, writing in matrix form, then solving, gives us the optimal set of parameters.</p>      
</div>
</section>

<section>
  <div class="inner_cell">
    <h3 id="Fitting a Quadratic function">Fitting a Quadratic function<a class="anchor-link" href="#Fitting a Quadratic function">&#182;</a></h3>
    <p>In the case that the largest power of $x$ is $x^2$ we have
     $$
     y_i = a_0 + a_1 x_i + a_2 x_i^2 + e_i
     $$
     and an overall error function
     $$
     S_r = \sum_{i=0}^{N-1} (y_i - a_0 - a_1 x_i - a_2 x_i^2)^2
     $$
   </p>
   <p>This leads to a set of equations
     \begin{align*}
     \left(n\right)a_0 + \left(\sum x_i\right) a_1 + \left(\sum x_i^2\right) a_2 & = \sum y_i \\
     \left(\sum x_i\right) a_0 + \left(\sum x_i^2\right) a_1 + \left(\sum x_i^3\right) a_2 & = \sum x_i y_i \\
     \left(\sum x_i^2\right) a_0  + \left(\sum x_i^3\right) a_1 + \left(\sum x_i^4\right) a_2 & = \sum x_i^2 y_i \\
     \end{align*}
   </p>

   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <li> Derive the above equations by differentiating $S_r$ with respect to each of the $a_i$ in turn.</li>	
   <li> Write the equations in matrix form $\textbf{A}x = b$, where $x$ is a column matrix with entries $a_0, a_1, a_2$ and $b$ is a column matrix with terms that do not depend on the fitting parameters, $a_0$, $a_1$  and  $a_2$. </li>
   <p>using the data you can find on Blackboard for today's session under the assessments tab."</p>
   <li> Solve for $a_0$, $a_1$  and  $a_2$.</li>
   <li> Plot your fitted parabola against the data to check the fit.</li>
 </div>      
</div>           
</section>

<section>
  <div class="inner_cell">
    <h2 id="Multiple Linear Regression">Multiple Linear Regression<a class="anchor-link" href="#Multiple Linear Regression">&#182;</a></h2>
    <p>Instead of powers of a single variable, our model for $y_i$ could be that it is a function of several independent variables:
     $$
     y_i = a_0 + a_1 x_{1i} + a_2 x_{2i} + a_3 x_{3i} + \ldots + a_n x_{ni}
     $$
   </p>

   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <li> Derive the a set of equations for the case of two independent variables</li>	
   <li> Write the equations in matrix form $Ax = b$, where $x$ is a column matrix with entries $a_0, a_1, a_2$ </li>
   <p>Using the following data
     $$
     \begin{array} {ccc}
     x_1 & x_2 & y \\
     \hline
     0 & 0 & 5 \\
     2 & 1 & 10 \\
     2.5 & 2 & 9 \\
     1 & 3 & 0 \\
     4 & 6 & 3 \\
     7 & 2 & 27 \\
     \hline
     \end{array}
     $$
   </p>

   <li> Solve for $a_0$, $a_1$  and  $a_2$</li>
   <li> Plot your fitted function against the data to check the fit</li>
 </div>                 
</div>
</section>

<section>
  <div class="inner_cell">
    <h3 id="Linearization of data sets">Linearization of data sets<a class="anchor-link" href="#Linearization of data sets">&#182;</a></h3>
    <p>Multiple Linear Regression is not just limited to obviously linear data.</p>
    <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
    8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
    <li> How would you apply multiple linear regression to data that you thought was related by $y = a_0 x_1 ^ {a_1} x_2 ^ {a_2} \cdots x_n ^ {a_n}$ ?</li>	
  </div>
</div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
    <p>Simple linear, polynomial and multiple linear regression can be generalised to the following linear least-squares model
     $$
     y = a_0 z_0 + a_1 z_1 + a_2 z_2 + \cdots + a_m z_m + e
     $$
     where $z_0, z_1, \ldots , z_m$ are $m+1$ $\textbf{basis functions}$.
   </p>
   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <p>What would $z_0, z_1, \ldots , z_m$ be for</p>
   <li>Simple linear least squares regression?</li> 
   <li>Polynomial least squares regression?</li> 
   <li>Multiple linear least squares regression?</li> 
 </div>

 <p>The linear refers to the parameters $a_0, a_1, \ldots, a_m$, the $z$s can be highly non-linear</p>
 <p>For instance.
   $$
   y = a_0 + a_1 \cos (\omega t) + a_2 \sin (\omega t)
   $$
   fits this model.
 </p>
</div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
    <p>We can rewrite
     $$
     y = a_0 z_0 + a_1 z_1 + a_2 z_2 + \cdots + a_m z_m + e
     $$
     in matrix notation as
     $$
     \{Y\} = [Z]\{A\} + \{E\}
     $$
     where $\{\}$ indicates a column vector, for clarity, and $[]$ indicates a matrix.
     $[Z]$ contains the calculated values of the $m+1$ basis functions at the $n+1$ measured values of the independent variables:
     $$
     [Z] =
     \left[
     \begin{matrix}
     z_{00} & z_{10} & \cdots & z_{m0} \\
     z_{01} & z_{11} & \cdots & z_{m1} \\
     \vdots & \vdots & \ddots & \vdots \\
     z_{0n} & z_{1n} & \cdots & z_{mn} \\
     \end{matrix}
     \right]
     $$
   </p>

   <p>
    The column vector $\{Y\}$ contains the $n+1$ observed values of the dependent variable
     $$
     \{Y\}^T = \left[y_0, y_1, y_2, y_3, \ldots, y_n  \right]
     $$
     The column vector $\{A\}$ contains the $m+1$ unknown parameters of the model
     $$
     \{A\}^T = \left[a_0, a_1, a_2, \ldots, a_{m}  \right]
     $$
     The column vector $\{E\}$ contains the $n+1$ observed residuals (errors)
     $$
     \{E\}^T = \left[e_0, e_1, e_2, e_3, \ldots, e_n  \right]
     $$

   </p>
 </div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
    <p>We can also express error in our model as a sum of the squares much like before:
     $$
     S_r = \sum_{i=0}^{n}\left(y_i - \sum_{j=0}^{m} a_j z_{ji}  \right)^2
     $$
     Again, $S_r$ is minimised by taking partial derivatives wrt $a_i$, and yields
     $$
     [[Z]^T[Z]]\{A\} = \{[Z^T]\{Y\}\}
     $$
     which is equivalent to the normal equations (set of simultaneous equations for $a_i$ we found previously).
     More details can of the derivation can be found <a href="http://fourier.eng.hmc.edu/e176/lectures/NM/node35.html">here</a>, though the notation is a little different.
   </p>
   <p>
     This set of equations can be solved using the methods of solving linear equations.
   </p><p>
     For now, it can be done brute force by calculating the matrix inverse of $[[Z]^T[Z]]$ or using Gauss elimination.
   </p>
   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <p>Try to redo the earlier fitting problems in this notation / method.</p>
 </div><br><br>
</div>
</section>


<section><section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares - Example">General Linear Least Squares - Example<a class="anchor-link" href="#General Linear Least Squares - Example">&#182;</a></h2>
    <p>
      Suppose we have 11 measurements at 
      $$
        x^T = -3. , -2.3, -1.6, -0.9, -0.2,  0.5,  1.2,  1.9,  2.6,  3.3,  4.
      $$
      with values
      $$
        y^T = 8.26383742,  6.44045188,  4.74903073,  4.5656476 ,  3.61011683,
        3.32743918,  2.9643915 ,  1.02239181,  1.09485138,  1.84053372,
        1.49110572
      $$
      <img src='../images/genRegData.png' width="400px">
    </p>
    <p>Let us fit it to a function of the form $y_i = a_0 + a_1 e^{-x} + a_2 e^{-2x} $</p>
  </div></section>
  <section><div class='inner_cell'>
    <p>Our $Z$ matrix has 3 columns - for the basis function corresponding to $a_0$, then $a_1$ ($e^{-x}$) and then the term corresponding to $a_2$ $e^{-2x}$. It will have 11 rows corresponding to the 11 measurements.
      <pre>
        Z = 
      [[  1.00000000e+00,   2.00855369e+01,   4.03428793e+02],
       [  1.00000000e+00,   9.97418245e+00,   9.94843156e+01],
       [  1.00000000e+00,   4.95303242e+00,   2.45325302e+01],
       [  1.00000000e+00,   2.45960311e+00,   6.04964746e+00],
       [  1.00000000e+00,   1.22140276e+00,   1.49182470e+00],
       [  1.00000000e+00,   6.06530660e-01,   3.67879441e-01],
       [  1.00000000e+00,   3.01194212e-01,   9.07179533e-02],
       [  1.00000000e+00,   1.49568619e-01,   2.23707719e-02],
       [  1.00000000e+00,   7.42735782e-02,   5.51656442e-03],
       [  1.00000000e+00,   3.68831674e-02,   1.36036804e-03],
       [  1.00000000e+00,   1.83156389e-02,   3.35462628e-04]]
      </pre>
    </p>
    <p>
    Then we set up the linear equation problem by forming \([Z]^T[Z]\) and $[Z]\{y\}$ and combine them to form the following augmented matrix
      <pre>
      [[  1.10000000e+01,   3.98805235e+01,   5.35475292e+02, 3.86526046e+01],
       [  3.98805235e+01,   5.35475292e+02,   9.23382518e+03, 2.57093542e+02],
       [  5.35475292e+02,   9.23382518e+03,   1.73292733e+05, 3.89173010e+03]]        
      </pre>
    </p>
  </div>
</section>
<section>
  <div class="inner_cell">
    <p>
    The solutions of this problem are 
    <pre>
      a = [ 2.13758951,  0.58605735, -0.01537541]
    </pre>
    This means that our final model for the data is
    $$
    y = 2.13758951 + 0.58605735e^{-x} - 0.01537541e^{-2x}
    $$
    </p>
    <p>
      <img src='../images/fittedModel.png' width="500px">
    </p>

  </div>
</section>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Statistical interpretation of least squares">Statistical interpretation of least squares<a class="anchor-link" href="#Statistical interpretation of least squares">&#182;</a></h2>
    <p>The matrix $[[Z]^T[Z]]^{-1}$ actually contains the variance (diagonal elements) and covariances (off-diagonal elements) of the $a_i$ so can be used to estimate the accuracy of the parameter estimation.</p>
    <p>We can use the Gauss Jordan method to find $[[Z]^T[Z]]^{-1}$.</p>
    <p>We will look into this in more detail next week.</p>
  </div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Summary and Further Reading">Summary and Further Reading<a class="anchor-link" href="#Summary and Further Reading">&#182;</a></h2>
    <p>You should be reading additional material to provide a solid background to what we do in class</p>
    <p>Reading: Capra and Canale, introduction to part 5 and chapter 17.</p>
    <p>Lots of details inchapters 14 and 15 of <a href="http://apps.nrbook.com/c/index.html">Numerical Recipes</a>.</p>
  </div>
</section>

</div> <!-- slides -->
</div> <!-- reveal -->

<script>

  require(
  {
      // it makes sense to wait a little bit when you are loading
      // reveal from a cdn in a slow connection environment
      waitSeconds: 15
    },
    [
    "../../../revealjs//lib/js/head.min.js",
    "../../../revealjs//js/reveal.js"
    ],

    function(head, Reveal){

      // Full list of configuration options available here: https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        mouseWheel: false,
        center: false,
        scroll: true,

        // Parallax background image
        parallaxBackgroundImage: 'https://upload.wikimedia.org/wikipedia/commons/2/20/Coloured_Voronoi_2D.svg',
        // Parallax background size
        parallaxBackgroundSize: '2100px 900px', // CSS syntax, e.g. "2100px 900px

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
        { src: "../../../revealjs//lib/js/classList.js",
        condition: function() { return !document.body.classList; } },
        { src: "../../../revealjs//plugin/notes/notes.js",
        async: true,
        condition: function() { return !!document.body.classList; } }
        ]
      });

      var update = function(event){
        if(MathJax.Hub.getAllJax(Reveal.getCurrentSlide())){
          MathJax.Hub.Rerender(Reveal.getCurrentSlide());
        }
      };

      Reveal.addEventListener('slidechanged', update);

      var update_scroll = function(event){
        $(".reveal").scrollTop(0);
      };

      Reveal.addEventListener('slidechanged', update_scroll);

    }
    );
  </script>

</body>

</html>
