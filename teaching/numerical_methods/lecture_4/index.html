
<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1" />

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <title>Numerical Methods - Curve Fitting 2</title>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

  <!-- General and theme style sheets -->
  <link rel="stylesheet" href="../../../revealjs/css/reveal.css">
  <link rel="stylesheet" href="../../../revealjs/css/theme/beige.css" id="theme">

  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
    if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = '../../../revealjs/css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    }

  </script>

  <!--[if lt IE 9]>
  <script src="../../../revealjs//lib/js/html5shiv.js"></script>
<![endif]-->

<!-- Loading the mathjax macro -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
  processEscapes: true,
  processEnvironments: true
},
// Center justify equations in code and markdown cells. Elsewhere
// we use CSS to left justify single line equations in code cells.
displayAlign: 'center',
"HTML-CSS": {
styles: {'.MathJax_Display': {"margin": 0}},
linebreaks: { automatic: true }
}
});
</script>
<!-- End of mathjax configuration -->

<!-- Get Font-awesome from cdn -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.css">

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="../custom.css">

</head>
<body>

  <div class="reveal">
    <div class="slides">

      <section data-background-image="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/2000px-Anscombe%27s_quartet_3.svg.png">
        <h2 style="color:white;" id="Numerical Methods Week 3">Numerical Methods Week 4<a class="anchor-link" href="#Numerical Methods Week 3">&#182;</a></h2>
        <h1 style="color:white;" id="Curve Fitting 2">Curve Fitting 2<a class="anchor-link" href="#Curve Fitting 2">&#182;</a></h1>
        <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
        8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
        <p>We continue with Curve Fitting. This week polynomial and multiple linear regression.</p>
        <p>Reading: Capra and Canale, introduction to part 5 and chapter 17.</p>
      </div>
      <br>
      <div style="background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
      8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
      <p>Learning outcomes:</p>
      <li> Extend the work on Linear Regression to polynomial and multiple variables.</li>
      <li> Combine C++, python and analytical solutions or other platforms.</li>
      <li> Check your code works correctly, via an external reference.</li>	
    </div>
    <br>
    <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
    8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">

    <h3>Matt Watkins mwatkins@lincoln.ac.uk<a class="anchor-link" href="#Matt Watkins">&#182;</a></h3>
  </div>
</section>

<section>
  <div class="inner_cell">
    <h3 id="Feedback: Fitting a Quadratic function">Fitting a Quadratic function<a class="anchor-link" href="#Fitting a Quadratic function">&#182;</a></h3>
    <p>In the case that the largest power of $x$ is $x^2$ we have
     $$
     y_i (a_0, a_1, a_2; x_i) = a_0 + a_1 x_i + a_2 x_i^2 + e_i
     $$
     and an overall error function
     $$
     S_r (a_0, a_1, a_2) = \sum_{i=0}^{N-1} (y_i - a_0 - a_1 x_i - a_2 x_i^2)^2
     $$
   </p>
   <p>This leads to a set of equations
     \begin{align*}
     \frac{\delta S_r (a_0, a_1, a_2)}{\delta a_0} & \implies \left(n\right)a_0 + \left(\sum x_i\right) a_1 + \left(\sum x_i^2\right) a_2 & = \sum y_i \\
     \frac{\delta S_r (a_0, a_1, a_2)}{\delta a_1} & \implies \left(\sum x_i\right) a_0 + \left(\sum x_i^2\right) a_1 + \left(\sum x_i^3\right) a_2 & = \sum x_i y_i \\
     \frac{\delta S_r (a_0, a_1, a_2)}{\delta a_2} & \implies \left(\sum x_i^2\right) a_0  + \left(\sum x_i^3\right) a_1 + \left(\sum x_i^4\right) a_2 & = \sum x_i^2 y_i \\
     \end{align*}
   </p>

   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <li> Derive the above equations by differentiating $S_r$ with respect to each of the $a_i$ in turn.</li>	
   <li> Write the equations in matrix form $\textbf{A}x = b$, where $x$ is a column matrix with entries $a_0, a_1, a_2$ and $b$ is a column matrix with terms that do not depend on the fitting parameters, $a_0$, $a_1$  and  $a_2$. </li>
   <li> Solve for $a_0$, $a_1$  and  $a_2$.</li>
   <li> Plot your fitted parabola against the data to check the fit.</li>
 </div>      
</div>           
</section>

<section>
  <div class="inner_cell">
    <h3 id="Feedback: Fitting a Quadratic function">Fitting a Quadratic function<a class="anchor-link" href="#Fitting a Quadratic function">&#182;</a></h3>
    <p>This leads to a set of equations
     \begin{align*}
     \frac{\delta S_r (a_0, a_1, a_2)}{\delta a_0} & \implies \left(n\right)a_0 + \left(\sum x_i\right) a_1 + \left(\sum x_i^2\right) a_2 & = \sum y_i \\
     \frac{\delta S_r (a_0, a_1, a_2)}{\delta a_1} & \implies \left(\sum x_i\right) a_0 + \left(\sum x_i^2\right) a_1 + \left(\sum x_i^3\right) a_2 & = \sum x_i y_i \\
     \frac{\delta S_r (a_0, a_1, a_2)}{\delta a_2} & \implies \left(\sum x_i^2\right) a_0  + \left(\sum x_i^3\right) a_1 + \left(\sum x_i^4\right) a_2 & = \sum x_i^2 y_i \\
     \end{align*}
   </p>
   <p>
    We can rewrite these equations as
    $$
    \begin{pmatrix}
    \left(\sum x_i^0\right) & \left(\sum x_i\right)  & \left(\sum x_i^2\right) \\ 
    \left(\sum x_i\right)  & \left(\sum x_i^2\right) & \left(\sum x_i^3\right) \\ 
    \left(\sum x_i^2\right)   & \left(\sum x_i^3\right)  & \left(\sum x_i^4\right) \\ 
    \end{pmatrix}
    \begin{pmatrix}
    a_0 \\
    a_1 \\
    a_2 
    \end{pmatrix} =
    \begin{pmatrix}
    \sum y_i \\
    \sum x_i y_i \\
    \sum x_i^2 y_i 
    \end{pmatrix}
    $$
  </p>
  <p> Which is of the form $\textbf{A}x = b$</p>
</div>      
</section>


<section>
  <div class="inner_cell">
    <h2 id="Multiple Linear Regression">Multiple Linear Regression<a class="anchor-link" href="#Multiple Linear Regression">&#182;</a></h2>
    <p>Instead of powers of a single variable, our model for $y_i$ could be that it is a function of several independent variables:
     $$
     y_i(a_0, a_1, a_2, a_3 \ldots a_n; x_0, x_1, x_2, x_3 \ldots x_n) = a_0 + a_1 x_{1i} + a_2 x_{2i} + a_3 x_{3i} + \ldots + a_n x_{ni}
     $$
   </p>

   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <li> Derive the a set of equations for the case of two independent variables</li>	
   <li> Write the equations in matrix form $Ax = b$, where $x$ is a column matrix with entries $a_0, a_1, a_2$ </li>
   <p>Using the following data
     $$
     \begin{array} {ccc}
     x_1 & x_2 & y \\
     \hline
     0 & 0 & 5 \\
     2 & 1 & 10 \\
     2.5 & 2 & 9 \\
     1 & 3 & 0 \\
     4 & 6 & 3 \\
     7 & 2 & 27 \\
     \hline
     \end{array}
     $$
   </p>

   <li> Solve for $a_0$, $a_1$  and  $a_2$</li>
   <li> Plot your fitted function against the data to check the fit</li>
 </div>                 
</div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
    <p>Simple linear, polynomial and multiple linear regression can be generalised to the following linear least-squares model
     $$
     y_i (a_0, a_1, a_2, a_3 \ldots a_n; x_0, x_1, x_2, x_3 \ldots x_n) = a_0 z_0 + a_1 z_1 + a_2 z_2 + \cdots + a_m z_m + e
     $$
     are now not just coordinates $x_i$ but some <em>predefined</em> functions of those positions $z_0(x_0, x_1, x_2, x_3 \ldots x_n), z_1( x_0, x_1, x_2, x_3 \ldots x_n), \ldots , z_m( x_0, x_1, x_2, x_3 \ldots x_n)$ are $m+1$ $\textbf{basis functions}$.
   </p>

   <p>The linear refers to the parameters $a_0, a_1, \ldots, a_m$, the $z$s can be highly non-linear</p>
   <p>For instance.
     $$
     y(a_0, a_1, a_2; x_0) = a_0 + a_1 \cos (\omega x_0) + a_2 \sin (\omega x_0)
     $$
     fits this model with $z_0 = 1, z_1 = \cos(\omega x_0)$ and $z_2 = \sin(\omega x_0)$. Where $\omega$ is a single independent variable and $\omega$ is a predefined constant.</p>
     <p> I'll supress the functional dependence notation as it gets too much. Remember the $z$s are predefined - only the $a$s are optimized.
   </p>
 </div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
    <p>We can rewrite
     $$
     y = a_0 z_0 + a_1 z_1 + a_2 z_2 + \cdots + a_m z_m + e
     $$
     in matrix notation as
     $$
     \{Y\} = [Z]\{A\} + \{E\}
     $$
     where $\{\}$ indicates a column vector, for clarity, and $[]$ indicates a matrix.
     $[Z]$ contains the calculated values of the $m+1$ basis functions at the $n+1$ measured values of the independent variables:
     $$
     [Z] =
     \left[
     \begin{matrix}
     z_{00} & z_{10} & \cdots & z_{m0} \\
     z_{01} & z_{11} & \cdots & z_{m1} \\
     \vdots & \vdots & \ddots & \vdots \\
     z_{0n} & z_{1n} & \cdots & z_{mn} \\
     \end{matrix}
     \right]
     $$
   </p>

   <p>
    The column vector $\{Y\}$ contains the $n+1$ observed values of the dependent variable
    $$
    \{Y\}^T = \left[y_0, y_1, y_2, y_3, \ldots, y_n  \right]
    $$
    The column vector $\{A\}$ contains the $m+1$ unknown parameters of the model
    $$
    \{A\}^T = \left[a_0, a_1, a_2, \ldots, a_{m}  \right]
    $$
    The column vector $\{E\}$ contains the $n+1$ observed residuals (errors)
    $$
    \{E\}^T = \left[e_0, e_1, e_2, e_3, \ldots, e_n  \right]
    $$

  </p>
</div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
    <p>We can also express error in our model as a sum of the squares much like before:
     $$
     S_r = \sum_{i=0}^{n}\left(y_i - \sum_{j=0}^{m} a_j z_{ji}  \right)^2
     $$
     Again, $S_r$ is minimised by taking partial derivatives wrt $a_i$, and yields
     $$
     [[Z]^T[Z]]\{A\} = \{[Z^T]\{Y\}\}
     $$
     which is equivalent to the normal equations (set of simultaneous equations for $a_i$ we found previously).
     More details can of the derivation can be found <a href="http://fourier.eng.hmc.edu/e176/lectures/NM/node35.html">here</a>, though the notation is a little different.
   </p>
   <p>
     This set of equations can be solved using the methods of solving linear equations.
   </p><p>
     For now, it can be done brute force by calculating the matrix inverse of $[[Z]^T[Z]]$ or using Gauss elimination.
   </p>
   <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
   8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
   <p>Try to redo the earlier fitting problems in this notation / method.</p>
 </div><br><br>
</div>
</section>


<section><section>
  <div class="inner_cell">
    <h2 id="General Linear Least Squares - Example">General Linear Least Squares - Example<a class="anchor-link" href="#General Linear Least Squares - Example">&#182;</a></h2>
    <p>
      Suppose we have 11 measurements at 
      $$
      x^T = -3. , -2.3, -1.6, -0.9, -0.2,  0.5,  1.2,  1.9,  2.6,  3.3,  4.
      $$
      with values
      $$
      y^T = 8.26383742,  6.44045188,  4.74903073,  4.5656476 ,  3.61011683,
      3.32743918,  2.9643915 ,  1.02239181,  1.09485138,  1.84053372,
      1.49110572
      $$
      <img src='../images/genRegData.png' width="400px">
    </p>
    <p>Let us fit it to a function of the form $y_i = a_0 + a_1 e^{-x} + a_2 e^{-2x} $</p>
  </div></section>
  <section><div class='inner_cell'>
    <p>Our $Z$ matrix has 3 columns - for the basis function corresponding to $a_0$, then $a_1$ ($e^{-x}$) and then the term corresponding to $a_2$ $e^{-2x}$. It will have 11 rows corresponding to the 11 measurements.
      <pre>
        Z = 
        [[  1.00000000e+00,   2.00855369e+01,   4.03428793e+02],
        [  1.00000000e+00,   9.97418245e+00,   9.94843156e+01],
        [  1.00000000e+00,   4.95303242e+00,   2.45325302e+01],
        [  1.00000000e+00,   2.45960311e+00,   6.04964746e+00],
        [  1.00000000e+00,   1.22140276e+00,   1.49182470e+00],
        [  1.00000000e+00,   6.06530660e-01,   3.67879441e-01],
        [  1.00000000e+00,   3.01194212e-01,   9.07179533e-02],
        [  1.00000000e+00,   1.49568619e-01,   2.23707719e-02],
        [  1.00000000e+00,   7.42735782e-02,   5.51656442e-03],
        [  1.00000000e+00,   3.68831674e-02,   1.36036804e-03],
        [  1.00000000e+00,   1.83156389e-02,   3.35462628e-04]]
      </pre>
    </p>
    <p>
      Then we set up the linear equation problem by forming \([Z]^T[Z]\) and $[Z]^T\{y\}$ and combine them to form the following augmented matrix
      <pre>
        [[  1.10000000e+01,   3.98805235e+01,   5.35475292e+02, 3.86526046e+01],
        [  3.98805235e+01,   5.35475292e+02,   9.23382518e+03, 2.57093542e+02],
        [  5.35475292e+02,   9.23382518e+03,   1.73292733e+05, 3.89173010e+03]]        
      </pre>
    </p>
  </div>
</section>
<section>
  <div class="inner_cell">
    <p>
      The solutions of this problem are 
      <pre>
        a = [ 2.13758951,  0.58605735, -0.01537541]
      </pre>
      This means that our final model for the data is
      $$
      y = 2.13758951 + 0.58605735e^{-x} - 0.01537541e^{-2x}
      $$
    </p>
    <p>
      <img src='../images/fittedModel.png' width="500px">
    </p>

  </div>
</section>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Statistical interpretation of least squares">Statistical interpretation of least squares<a class="anchor-link" href="#Statistical interpretation of least squares">&#182;</a></h2>
    <p>The matrix $[[Z]^T[Z]]^{-1}$ contains the variance (diagonal elements) and covariances (off-diagonal elements) of the $a_i$ so can be used to estimate the accuracy of the parameter estimation.</p>
    <p>We can use the Gauss Jordan method to find $[[Z]^T[Z]]^{-1}$.</p>
    <p>The diagonal elements of $[[Z]^T[Z]]^{-1}$ can be designated as $z_{i,i}^{-1}$</p>
    <p>We will call the standard deviation of our fitted model to the data $s_{y/x} = \frac{1}{\sqrt{n-1}}\sqrt{\sum_{i=0}^{i=n} (y_i -  a_0 z_{0i} + a_1 z_{1i} + a_2 z_{2i} + \cdots + a_m z_{mi} )^2}$</p>
    <p>The variances of our parameters are given by $\text{var}(a_i) = s^2(a_i)= z_{i,i}^{-1} s_{y/x}^2$</p>
  </div>
</section>

<section>
  <div class='inner_cell'>
    <p>We can now place error limits on our optimal parameters, $a_0, ... a_m$.</p>
    <p>If our model is good, the real data should be approximately normally distributed around our model. You can the show that the parameters should have a <a href="https://mattatlincoln.github.io/teaching/statistics/lecture_9/#/6/2">t-distribution</a> with $n-2$ degrees of freedom.</p>
    <p>We can put confidence limits on the parameters using $P(\text{true value of the ith parameter is in the interval } (a_i - t_{c/2,n-1} s(a_i) , a_i + t_{c/2,n-1} s(a_i) ) = c$ where $c$ is our confidence, for instance 0.95 to be 95% certain the parameter lies within those bounds.</p>
  </div>
</section>

<section>
  <div class="inner_cell">
    <h2 id="Summary and Further Reading">Summary and Further Reading<a class="anchor-link" href="#Summary and Further Reading">&#182;</a></h2>
    <p>You should be reading additional material to provide a solid background to what we do in class</p>
    <p>Reading: Capra and Canale, introduction to part 5 and chapter 17.</p>
    <p>Lots of details inchapters 14 and 15 of <a href="http://apps.nrbook.com/c/index.html">Numerical Recipes</a>.</p>
  </div>
</section>

</div> <!-- slides -->
</div> <!-- reveal -->

<script>

  require(
  {
      // it makes sense to wait a little bit when you are loading
      // reveal from a cdn in a slow connection environment
      waitSeconds: 15
    },
    [
    "../../../revealjs//lib/js/head.min.js",
    "../../../revealjs//js/reveal.js"
    ],

    function(head, Reveal){

      // Full list of configuration options available here: https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        mouseWheel: false,
        center: false,
        scroll: true,

        // Parallax background image
        parallaxBackgroundImage: 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/2000px-Anscombe%27s_quartet_3.svg.png',
        // Parallax background size
        parallaxBackgroundSize: '2000px 1455px', // CSS syntax, e.g. "2100px 900px

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
        { src: "../../../revealjs//lib/js/classList.js",
        condition: function() { return !document.body.classList; } },
        { src: "../../../revealjs//plugin/notes/notes.js",
        async: true,
        condition: function() { return !!document.body.classList; } }
        ]
      });

      var update = function(event){
        if(MathJax.Hub.getAllJax(Reveal.getCurrentSlide())){
          MathJax.Hub.Rerender(Reveal.getCurrentSlide());
        }
      };

      Reveal.addEventListener('slidechanged', update);

      var update_scroll = function(event){
        $(".reveal").scrollTop(0);
      };

      Reveal.addEventListener('slidechanged', update_scroll);

    }
    );
  </script>

</body>

</html>
