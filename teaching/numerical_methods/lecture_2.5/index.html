
<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1" />

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <title>Numerical Methods - Curve Fitting 2</title>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

  <!-- General and theme style sheets -->
  <link rel="stylesheet" href="../../../revealjs/css/reveal.css">
  <link rel="stylesheet" href="../../../revealjs/css/theme/beige.css" id="theme">

  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
  if( window.location.search.match( /print-pdf/gi ) ) {
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = '../../../revealjs/css/print/pdf.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  }

  </script>

  <!--[if lt IE 9]>
  <script src="../../../revealjs//lib/js/html5shiv.js"></script>
  <![endif]-->

  <!-- Loading the mathjax macro -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <!-- Load mathjax -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
  <!-- MathJax configuration -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
      styles: {'.MathJax_Display': {"margin": 0}},
      linebreaks: { automatic: true }
    }
  });
  </script>
  <!-- End of mathjax configuration -->

  <!-- Get Font-awesome from cdn -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.css">

  <!-- Custom stylesheet, it must be in the same directory as the html file -->
  <link rel="stylesheet" href="../custom.css">

</head>
<body>

<div class="reveal">
  <div class="slides">

    <section data-background-image="https://upload.wikimedia.org/wikipedia/commons/2/20/Coloured_Voronoi_2D.svg">
      <h2 style="color:white;" id="Numerical Methods Week 3">Numerical Methods Week 3<a class="anchor-link" href="#Numerical Methods Week 3">&#182;</a></h2>
      <h1 style="color:white;" id="Curve Fitting 1">Curve Fitting 1<a class="anchor-link" href="#Curve Fitting 1">&#182;</a></h1>
      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
	
	<p>We continue with Curve Fitting. This week polynomial and multiple linear regression.</p>
	<p>Learning outcomes:</p>
      </div>
      <br>
      <div style="background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
        <li> Extend the work on Linear Regression to polynomial and multiple variables.</li>
        <li> Combine C++ and analytical or other platforms.</li>
        <li> Check your code works correctly, via an external reference.</li>	
      </div>
      <br>
      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
 
       <h3>Matt Watkins mwatkins@lincoln.ac.uk<a class="anchor-link" href="#Matt Watkins">&#182;</a></h3>
      </div>
    </section>

    <section>
      <h2 id="Least-squares Regression - summary">Least Squares Regression  - summary<a class="anchor-link" href="#Least Squares Regression - summary">&#182;</a></h2>
      <p> we saw in <a href='https://mattatlincoln.github.io/teaching/numerical_methods/lecture_1/#/'>lecture 1</a> that given our assumption of a straightline
	$$
	y_i = a_0 + a_1 x_i + e_i
	$$
	the error at each point is given by
	$$
	e_i  = y_i - a_0 - a_1 x_i
	$$
      </p>
      <p> We take the sum of the squares of the errors
	$$
	S_r = \sum_{i=0}^{n-1} e_i^2 = \sum_{i=0}^{n-1}(y_i - a_0 - a_1 x_i)^2
	$$
	as our error criterion.
      </p>
      <p> We can find an optimal $a_1$ and $a_0$.
	$$
	a_1 = \frac{n \sum x_i y_i - \sum x_i \sum y_i}{n \sum x_i^2 - (\sum x_i)^2}
	$$
	and 
	$$
	a_0 = \frac{\sum y_i}{n} - a_1 \frac{\sum x_i}{n} = \bar{y} - a_1 \bar{x}
	$$
	where $\bar{y}$ and $\bar{x}$ are the means of the $x$ and $y$ values.</p>
      <p> Correlation of the data: covariance of the data $(x,y)$ divided by the std of $x$ and $y$.
	$$
	r = \frac{n \sum x_i y_i - (\sum x_i)(\sum y_i)}{\sqrt{n \sum x_i^2 - (\sum x_i)^2}\sqrt{n \sum y_i^2 - (\sum y_i)^2}}
	$$
      </p>      
    </section>

    <section>
      <h2 id="Array bounds">Array bounds<a class="anchor-link" href="#Array bounds">&#182;</a></h2>
      <p> This code example goes wrong because it tries to access x[10]</p>
      <p> C arrays start from element [0] and go up to [n-1].</p>
      <p> So in the code below x[0], x[1], ... x[9] are assigned. </p>
      <p> But in the loop, x[10] just prints whatever happens to exist in that memory location. </p>
      <pre data-lang="c++">
	<code class='c++'>
// Be careful of array bounds
	  
#include "stdafx.h"
#include&ltiostream>

int main()
{
    // assign an array of length 10
    double x[10] = {0,1,2,3,4,5,6,7,8,9};

    // loop over array elements, be very careful!
    // array elements not assigned will contain random data
    // visual studio may complain (depends on version, it seems)
    for (int i = 0; i < 11; i++) {
        std::cout << "i = " << i << ", x[" << i << "] = " << x[i] << "\n";
    }
}	  
         </code>
      </pre>
    </section>

    <section>
      <h2 id="Polynomial Regression">Polynomial Least-squares Regression<a class="anchor-link" href="#Polynomial Regression">&#182;</a></h2>
      <p>We can easily extend our method to deal with polynomicals:
	$$
	y_i = a_0 + a_1 x_i + a_2 x_i^2 + \ldots + a_n x_i^n  + e_i
	$$
      </p>
      <p>We then take the derivatives wrt to the parameters ($a_0 \ldots a_n$) to get a set of equations.</p>
      <p>Setting these equations equal to zero then solving gives us the optimal set of parameters.</p>      
    </section>

    <section>
      <h2 id="Fitting a Quadratic function">Fitting a Quadratic function<a class="anchor-link" href="#Fitting a Quadratic function">&#182;</a></h2>
      <p>In the case that the largest power of $x$ is $x^2$ we have
      	$$
	y_i = a_0 + a_1 x_i + a_2 x_i^2 + e_i
	$$
      </p>
      <p>This leads to a set of equations
	\begin{align*}
	\left(n\right)a_0 + \left(\sum x_i\right) a_1 + \left(\sum x_i^2\right) a_2 & = \sum y_i \\
	\left(\sum x_i\right) a_0 + \left(\sum x_i^2\right) a_1 + \left(\sum x_i^3\right) a_2 & = \sum x_i y_i \\
	\left(\sum x_i^2\right) a_0  + \left(\sum x_i^3\right) a_1 + \left(\sum x_i^4\right) a_2 & = \sum x_i^2 y_i \\
	\end{align*}
      </p>

      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
        <li> Derive the above equations</li>	
        <li> Write the equations in matrix form $\textbf{A}x = b$, where $x$ is a column matrix with entries $a_0, a_1, a_2$ </li>
	<p>using the data you can find on Blackboard for today's session called "Curve Fitting 2"</p>
	<li> Solve for $a_0$, $a_1$  and  $a_2$</li>
	<li> Plot your fitted parabola against the data to check the fit</li>
      </div>                 
    </section>

    <section>
      <h2 id="Multiple Linear Regression">Multiple Linear Regression<a class="anchor-link" href="#Multiple Linear Regression">&#182;</a></h2>
      <p>Instead of powers of a single variable, our model for $y_i$ could be that it is a function of several independent variables:
	$$
	y_i = a_0 + a_1 x_{1i} + a_2 x_{2i} + a_3 x_{3i} + \ldots + a_n x_{ni}
	$$
      </p>
      
      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
        <li> Derive the a set of equations for the case of two independent variables</li>	
        <li> Write the equations in matrix form $Ax = b$, where $x$ is a column matrix with entries $a_0, a_1, a_2$ </li>
	<p>Using the following data
	  $$
	  \begin{array} {ccc}
	  x_1 & x_2 & y \\
	  \hline
	  0 & 0 & 5 \\
	  2 & 1 & 10 \\
	  2.5 & 2 & 9 \\
	  1 & 3 & 0 \\
	  4 & 6 & 3 \\
	  7 & 2 & 27 \\
	  \hline
	  \end{array}
	  $$
	</p>

	<li> Solve for $a_0$, $a_1$  and  $a_2$</li>
	<li> Plot your fitted function against the data to check the fit</li>
      </div>                 

    </section>

    <section>
      <h3 id="Linearization of data sets">Linearization of data sets<a class="anchor-link" href="#Linearization of data sets">&#182;</a></h3>
      <p>Multiple Linear Regression is not just limited to obviously linear data.</p>
      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
        <li> How would you apply multiple linear regression to data that you thought was related by $y = a_0 x_1 ^ {a_1} x_2 ^ {a_2} \cdots x_n ^ {a_n}$ ?</li>	
      </div>
    </section>

    <section>
      <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
      <p>Simple linear, polynomial and multiple linear regression can be generalised to the following linear least-squares model
	$$
	y = a_0 z_0 + a_1 z_1 + a_2 z_2 + \cdots + a_m z_m + e
	$$
	where $z_0, z_1, \ldots , z_m$ are $m+1$ $\textbf{basis functions}$.
      </p>
      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
	<p>What would $z_0, z_1, \ldots , z_m$ be for</p>
        <li>Simple linear least squares regression?</li> 
        <li>Polynomial least squares regression?</li> 
        <li>Multiple linear least squares regression?</li> 
      </div>

      <p>The linear refers to the parameters $a_0, a_1, \ldots, a_m$, the $z$s can be highly non-linear</p>
      <p>For instance.
	$$
	y = a_0 + a_1 \cos (\omega t) + a_2 \sin (\omega t)
	$$
	fits this model.
      </p>
    </section>

    <section>
      <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
      <p>We can rewrite
	$$
	y = a_0 z_0 + a_1 z_1 + a_2 z_2 + \cdots + a_m z_m + e
	$$
	in matrix notation as
	$$
	\{Y\} = [Z]\{A\} + \{E\}
	$$
	where $\{\}$ indicates a column vector, for clarity, and $[]$ indicates a matrix.
      </p>
      <p>
	$[Z]$ contains the calculated values of the $m+1$ basis functions at the $n$ measured values of the independent variables:
	$$
	[Z] =
	\left[
	\begin{matrix}
	z_{01} & z_{11} & \cdots & z_{m1} \\
	z_{02} & z_{12} & \cdots & z_{m2} \\
	\vdots & \vdots & \ddots & \vdots \\
	z_{0n} & z_{1n} & \cdots & z_{mn} \\
	\end{matrix}
	\right]
	$$
      </p>

      <p>
	The column vector $\{Y\}$ contains the $n$ observed values of the dependent variable
	$$
	\{Y\}^T = \left[y_1, y_2, y_3, \ldots, y_n  \right]
	$$
	The column vector $\{A\}$ contains the $m+1$ unknown parameters of the model
	$$
	\{A\}^T = \left[a_0, a_1, a_2, \ldots, a_{m}  \right]
	$$
	The column vector $\{E\}$ contains the $n$ observed residuals (errors)
	$$
	\{E\}^T = \left[e_1, e_2, e_3, \ldots, e_n  \right]
	$$
	
      </p>
    </section>

    <section>
      <h2 id="General Linear Least Squares">General Linear Least Squares<a class="anchor-link" href="#General Linear Least Squares">&#182;</a></h2>
      <p>We can also express the sum of the squares using the same form
	$$
	S_r = \sum_{i=1}^{n}\left(y_i - \sum_{j=0}^{m} a_j z_{ji}  \right)^2
	$$
	Again, $S_r$ is minimised by taking partial derivatives wrt $a_i$, and yields
	$$
	[[Z]^T[Z]]\{A\} = \{[Z^T]\{Y\}\}
	$$
	which is equivalent to the normal equations (set of simultaneous equations for $a_i$ we found previously).
      </p>
      <p>
	This set of equations can be solved using the methods of solving linear equations.
      </p><p>
	For now, it can be done brute force by calculating the matrix inverse of $[[Z]^T[Z]]$ or using Gauss elimination.
      </p>
      <div style="background-color:Lavender; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left:
		  8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;">
	<p>Try to redo the earlier fitting problems in this notation / method.</p>
      </div>

    </section>
    
    <section>
      <h2 id="Statistical interpretation of least squares">Statistical interpretation of least squares<a class="anchor-link" href="#Statistical interpretation of least squares">&#182;</a></h2>
      <p>The matrix $[[Z]^T[Z]]^{-1}$ actually contains the variance (diagonal elements) and covariances (off-diagonal elements) of the $a_i$ so can be used to estimate the accuracy of the parameter estimation.</p>
      <p>We can use the Gauss Jordan method to find $[[Z]^T[Z]]^{-1}$.</p>
    </section>

    <section>
      <h2 id="Summary and Further Reading">Summary and Further Reading<a class="anchor-link" href="#Summary and Further Reading">&#182;</a></h2>
      <p>You should be reading additional material to provide a solid background to what we do in class</p>
      <p>All the textbooks contain sections on least-squares fitting,<br> for instance chapters 14 and 15 of <a href="http://apps.nrbook.com/c/index.html">Numerical Recipes</a>.</p>
    </section>
    
  </div> <!-- slides -->
</div> <!-- reveal -->

  <script>

  require(
    {
      // it makes sense to wait a little bit when you are loading
      // reveal from a cdn in a slow connection environment
      waitSeconds: 15
    },
    [
      "../../../revealjs//lib/js/head.min.js",
      "../../../revealjs//js/reveal.js"
    ],

    function(head, Reveal){

      // Full list of configuration options available here: https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        mouseWheel: false,
        center: false,
        scroll: true,

        // Parallax background image
        //parallaxBackgroundImage: 'https://github.com/mattatlincoln/mattatlincoln.github.io/blob/master/images/CaF2_zoom.jpg?raw=true',
        // Parallax background size
        parallaxBackgroundSize: '2100px 900px', // CSS syntax, e.g. "2100px 900px

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: "../../../revealjs//lib/js/classList.js",
          condition: function() { return !document.body.classList; } },
          { src: "../../../revealjs//plugin/notes/notes.js",
          async: true,
          condition: function() { return !!document.body.classList; } }
        ]
      });

      var update = function(event){
        if(MathJax.Hub.getAllJax(Reveal.getCurrentSlide())){
          MathJax.Hub.Rerender(Reveal.getCurrentSlide());
        }
      };

      Reveal.addEventListener('slidechanged', update);

      var update_scroll = function(event){
        $(".reveal").scrollTop(0);
      };

      Reveal.addEventListener('slidechanged', update_scroll);

    }
  );
  </script>

</body>

</html>
