{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slides for Proabability and Statistics module, 2016-2017\n",
    "# Matt Watkins, University of Lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Covariance and Correlation\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "- covariance between two variables\n",
    "- correlation between random variables\n",
    "- expectation and variance of sums of independent random variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joint random variables.\n",
    "\n",
    "The joint probability mass/density function contains all the information about the variables it describes.\n",
    "\n",
    "We've just seen that we can retrieve the individual probability mass/density functions by integrating/summing over the other variables. \n",
    "\n",
    "If we have a general function of the variables, $g(X,Y)$, say, then we can obtain the expectation value of that function as\n",
    "\n",
    "$$\n",
    "\\text{E}[g(X,Y)] = \\sum_{y} \\sum_{x} g(x,y)p(x,y)\n",
    "$$\n",
    "\n",
    "for discrete random variables and \n",
    "\n",
    "$$\n",
    "\\text{E}[g(X,Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y)f(x,y) \\mathrm{d}x \\mathrm{d}y\n",
    "$$\n",
    "\n",
    "there are proofs of this in Ross."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variances of Sums of Random variables.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "**if and only if the two variables are independent.**\n",
    "\n",
    "Or in general if $n$ variables are mututally independent. This formula is actually a special case of something called the **covariance** between two random variables.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "the covariance between $X$ and $Y$, denoted by $\\text{Cov}(X,Y)$, is defined by\n",
    "$$\n",
    "\\text{Cov}(X,Y) = \\text{E}[(X - \\text{E}[X])(Y - \\text{E}[Y])]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**example**\n",
    "back to our coins,\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X_1,X_2) = \\text{E}[(X_1 - \\text{E}[X_1])(X_2 - \\text{E}[X_2])] = \\\\\n",
    "\\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 - \\mu_{X_1} ) \\cdot (x_2 - \\mu_{X_2} )\\cdot p(x_1, x_2) = \\\\\n",
    "$$\n",
    "\n",
    "were the second line comes from the general expression\n",
    "\n",
    "$$\n",
    "\\text{E}[g(X,Y)] = \\sum_{y} \\sum_{x} g(x,y)p(x,y)\n",
    "$$\n",
    "\n",
    "adjusted to our particular case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X_1,X_2) = \\text{E}[(X_1 - \\text{E}[X_1])(X_2 - \\text{E}[X_2])] = \\\\\n",
    "\\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 - \\mu_{X_1} ) \\cdot (x_2 - \\mu_{X_2} )\\cdot p(x_1, x_2) = \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case we can very reasonably expect that $p(x_1,x_2) = p(x_1)p(x_2)$ (remember the definition of independent probabilities). We can now write out all the terms in the double sum:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 - \\mu_{X_1} ) \\cdot (x_2 - \\mu_{X_2} )\\cdot p(x_1, x_2) = \\\\\n",
    "(0-\\frac{1}{2})\\times(0-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1-\\frac{1}{2})\\times(0-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(0-\\frac{1}{2})\\times(1-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1-\\frac{1}{2})\\times(1-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} = \\\\\n",
    "\\frac{1}{16} - \\frac{1}{16} -\\frac{1}{16} + \\frac{1}{16} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "this will be the case whenever the two variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful formula\n",
    "\n",
    "In a similar way to the variance the covariance can more easily be caluclated using an alternative expression\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,Y) = \\text{E}[(X - \\text{E}[X])(Y - \\text{E}[Y])] = \\\\\n",
    "\\text{E}[XY - \\text{E}[X]Y - X\\text{E}[Y] + \\text{E}[X]\\text{E}[Y]] = \\\\\n",
    "\\text{E}[XY] - \\text{E}[X]\\text{E}[Y] - \\text{E}[X]\\text{E}[Y] + \\text{E}[X]\\text{E}[Y] = \\\\\n",
    "\\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can also see that the independence of $X$ and $Y$ guarantees that $\\text{Cov}(X,Y)=0$ from this expression by using the result that\n",
    "\n",
    "$$\n",
    "\\text{E}[XY] = \\text{E}[X]\\text{E}[Y]\n",
    "$$\n",
    "\n",
    "for independent variables. Again we can check that this holds for our $X_1$ and $X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "calculate $\\text{E}[X_1 X_2]$ and $\\text{Cov}(X_1,X_2)$ for our two coin example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[X_1 X_2] = \\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 ) \\cdot (x_2 )\\cdot p(x_1, x_2) = \\\\\n",
    "\\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 ) \\cdot (x_2 )\\cdot p(x_1) \\cdot p(x_2) = \\\\\n",
    "(0)\\times(0) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1)\\times(0) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(0)\\times(1) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1)\\times(1) \\times \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "looking back we see that this is indeed equal to $\\text{E}[X_1]\\text{E}[X_2]$, so again\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X_1,X_1) = \\text{E}[(X_1 - \\text{E}[X_1])(X_2 - \\text{E}[X_2])] = \\\\\n",
    "\\text{E}[X_1 X_2] - \\text{E}[X_1]\\text{E}[X_2] = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation\n",
    "\n",
    "In general it can be shown that a positive $\\text{Cov}(X,Y)$ is an indication that $Y$ increases when $X$ does.\n",
    "\n",
    "A negative $\\text{Cov}(X,Y)$ is an indication that $Y$ decreases when $X$ increases. \n",
    "\n",
    "What we want is a better indicator than the covariance. \n",
    "\n",
    "The difficulty with the covariance is that it has dimensions, and its size is dependent on the definition of the variables. Instead, or in addition, we can use the correlation.\n",
    "\n",
    "---\n",
    "\n",
    "**definition**\n",
    "\n",
    "$$\n",
    "\\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "It can be shown that the correlation will always have a value betwen $-1$ and $+1$. \n",
    "\n",
    "The significance of the correlation is similar to just discussed for the covariance: \n",
    "- Positive correlation between two variables means that $X$ increases as $Y$ increases. \n",
    "- Negative correlation means that $X$ decreases as $Y$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meaning of covariance\n",
    "\n",
    "It indicates whether there is a relationship between two random variables. \n",
    "\n",
    "We just saw that in the case of two independent random variables the covariance will be zero.\n",
    "\n",
    "Lets define two special random variables, $X$ and $Y$. \n",
    "\n",
    "They indicate whether or not events $A$ and $B$ occur. The are sometimes called indicator variables.\n",
    "\n",
    "In our language of random variables\n",
    "\n",
    "$$\n",
    "X = \\Bigg \\{ \\begin{array}{ll}\n",
    "1 & \\mbox{if $A$ occurs}\\\\\n",
    "0 & \\mbox{0 otherwise}\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "$$\n",
    "Y = \\Bigg \\{ \\begin{array}{ll}\n",
    "1 & \\mbox{if $B$ occurs}\\\\\n",
    "0 & \\mbox{0 otherwise}\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "and we can then see that\n",
    "\n",
    "$$\n",
    "XY = \\Bigg \\{ \\begin{array}{ll}\n",
    "1 & \\mbox{if $X=1$, $Y=1$}\\\\\n",
    "0 & \\mbox{0 otherwise}\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "So if we look at the covariance of $X$ and $Y$ we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,Y) & = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\\\\\n",
    "                & = P(\\{X=1, Y=1\\}) - P(\\{X=1\\})P(\\{Y=1\\})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "the last line comes from the definition of the variables, it is the proportion that the variable is in the 1 state.\n",
    "\n",
    "What if the covariance was greater than 0, what would that imply about the variables (and therefore the events $A$ and $B$)?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,Y) \\gt 0 \\implies P(\\{X=1, Y=1\\}) \\gt P(\\{X=1\\})P(\\{Y=1\\})\\\\\n",
    "\\implies \\frac{P(\\{X=1, Y=1\\})}{P(\\{X=1\\})} \\gt P(\\{Y=1\\})\\\\\n",
    "\\implies P(\\{Y=1 \\mid X=1\\}) \\gt P(\\{Y=1\\})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using the definition of a conditional probability in the last line.\n",
    "\n",
    "This means that if the event $A$ has happened (so $X=1$), it becomes more likely that event $B$ will also happen.\n",
    "\n",
    "Note that we could swap $X$ and $Y$ above, and it would imply that $B$ became more likely when $A$ has occured too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It might be that the covariance is negative. Then we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,Y) \\lt 0 & \\implies P(\\{X=1, Y=1\\}) \\gt P(\\{X=1\\})P(\\{Y=1\\})\\\\\n",
    "                      & \\implies \\frac{P(\\{X=1, Y=1\\})}{P(\\{X=1\\})} \\lt P(\\{Y=1\\})\\\\\n",
    "                      & \\implies P(\\{Y=1 \\mid X=1\\}) \\lt P(\\{Y=1\\})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So in this case event $A$ happening makes $B$ less likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance is a special case of covariance\n",
    "\n",
    "we also note that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,X) & = \\text{E}\\Big[(X - \\text{E}[X])\\cdot (X - \\text{E}[X])\\Big] \\\\\n",
    "                & = \\text{E}\\Big[(X - \\text{E}[X])^2\\Big] \\\\ \n",
    "                & = \\text{Var}(X)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "actually, the covariance is a matrix when there are many variables. \n",
    "\n",
    "The diagonal elements are the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "We have \n",
    "- defined the covariance of two random variables $X$ and $Y$\n",
    "- shown that $\\text{Cov}(X,Y) = 0$ for independent random variables.\n",
    "- defined the correlation between two random variables $X$ and $Y$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
