{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slides for Proabability and Statistics module, 2016-2017\n",
    "# Matt Watkins, University of Lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Covariance and Correlation\n",
    "\n",
    "last week we looked at the properties of expectations of random variables.\n",
    "\n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "By the end of this lecture you should know:\n",
    "<br><br>\n",
    "<li> how to calculate the expectation value of a jointly distributed discrete random variable  </li>\n",
    "<li> how to calculate the covariance and correlation of jointly distributed random variables.</li>\n",
    "<li> how to calculate expectation values and variances of sums of independent random variables</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expectation and Variance revisited\n",
    "\n",
    "- The expectation value gives the weighted average of the possible values of the random variable. The weighting is the likelihood that that value turns up. This is the **mean** of the random variable, often written as $\\mu$.\n",
    "\n",
    "Given a random variable $X$ with a probability mass function $p(x)$  we define the expectation of $X$, written as $\\text{E}[X]$ as \n",
    "\n",
    "$$\n",
    "\\text{E}[X] = \\sum_{\\text{all $x$}} x\\cdot p(x)\n",
    "$$\n",
    "\n",
    "The expectation of a discrete random variable $X$ is just the arithmetic mean of the values it takes on.\n",
    "\n",
    "The equivalent for a continuous random variable $Z$ is\n",
    "\n",
    "$$\n",
    "\\text{E}[Z] = \\int_{\\text{all $z$}} z\\cdot f(z) \\mathrm{d}z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**example**  \n",
    "let $X$ be the random variable 'number of heads' when two coins are flipped, what is the expectation of $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets write down the probability mass function of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(0)=\\frac{1}{4}, p(1)= \\frac{1}{2}, p(2)= \\frac{1}{4}$, then the expectation of $X$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\text{E}[X] = \\sum_{\\text{all $x$}} x\\cdot p(x) = \\\\\n",
    "0 \\cdot p(0) + 1 \\cdot p(1) + 2 \\cdot p(2) = \\\\\n",
    "0 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 2 \\cdot \\frac{1}{4} = 1 = \\mu_X\n",
    "$$\n",
    "\n",
    "this is the expected number of heads between two coins. From our initial discussions this should be the long-term average number of heads if the experiment was repeated many, many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**example**  \n",
    "In a game the player wins back a sum that is the 'square of the number of heads' on two coins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have a new random variable $Y$ with probability mass function\n",
    "\n",
    "$p(0)=\\frac{1}{4}, p(1)= \\frac{1}{2}, p(4)= \\frac{1}{4}$, then the expectation of $Y$ is given by\n",
    "\n",
    "$$ \\text{E}[Y] = \\sum_{\\text{all $y$}} y\\cdot p(y) = \\\\\n",
    "0 \\cdot p(0) + 1 \\cdot p(1) + 4 \\cdot p(4) = \\\\\n",
    "0 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 4 \\cdot \\frac{1}{4} = \\frac{3}{2} = \\mu_Y\n",
    "$$\n",
    "\n",
    "note that $Y$ = $X^2$.\n",
    "\n",
    "$$ \\text{E}[X^2] = \\sum_{\\text{all $x$}} x^2\\cdot p(2) = \\\\\n",
    "0 \\cdot p(0) + 1 \\cdot p(1) + 2^2 \\cdot p(2) = \\\\\n",
    "0 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 4 \\cdot \\frac{1}{4} = \\frac{3}{2} = \\mu_{X^2}\n",
    "$$\n",
    "\n",
    "This is the square of number of heads between two coins. From our initial discussions this should be the long-term average if the experiment was repeated many, many times. It may not be a possible value of $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition**\n",
    "\n",
    "Let $g(X)$ be any function of a random variable $X$. Then\n",
    "\n",
    "$$\n",
    "\\text{E}[g(X)] = \\sum_{\\text{all $x$}}g(x) \\cdot p(x)\n",
    "$$\n",
    "\n",
    "or for the continuous random variable $Z$\n",
    "\n",
    "$$\n",
    "\\text{E}[g(Z)] = \\int_{\\text{all $z$}} g(z)\\cdot f(z) \\mathrm{d}z\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "if $X$ is a random variable, then\n",
    "\n",
    "- $\\text{E}[a] = a$\n",
    "- $\\text{E}[aX] = a\\text{E}[X]$\n",
    "- $\\text{E}[g_1(X) + g_2(X)] = \\text{E}[g_1(X)] + \\text{E}[g_2(X)]$, where $g_1(X)$ and $g_2(X)$ are any functions of X. \n",
    "\n",
    "these define the properties of a linear operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition**\n",
    "\n",
    "If $X$ is a random variable with mean $\\mu$, then the variance of $X$, denoted by $\\text{Var}(X)$, is defined by\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\text{E}[(X-\\mu)^2]\n",
    "$$\n",
    "\n",
    "this can also be written\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2 \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "we saw in the problem class that\n",
    "\n",
    "$$\n",
    "\\text{Var}(aX + b) = a^2 \\text{Var}(X)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**example**\n",
    "the variance of $X$ - we know that it is given by\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\text{E}[X^2] - \\text{E}[X]^2 = \\frac{3}{2} - 1^2 = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "and we have a standard deviation of $\\sqrt{\\text{Var}(X)} = \\frac{1}{4}$.\n",
    "\n",
    "It is easier to see the meaning of the variance from the alternative form:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\text{E}[(X-\\mu_X)^2] =  \\sum_{\\text{all $x$}} (x - \\mu_X) ^2 \\cdot p(x) = \\\\\n",
    "(0 - 1)^2 \\cdot p(0) + (1 - 1)^2 \\cdot p(1) + (2 - 1)^2 \\cdot p(2) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "In agreement with the previous value.\n",
    "\n",
    "**It is the expected value of the square of the distance of $X$ to $\\mu_X$.**\n",
    "\n",
    "- If $X$ only took on its average value, the variance would be 0. \n",
    "- The closer the values of $x$ are to $\\mu$ the smaller the value of $\\text{var}(X)$. \n",
    "- The less likely values of $x$ far from $\\mu$ are, the smaller the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joint random variables.\n",
    "\n",
    "We defined two random variables - $X$ and $Y$. What about if it was important to consider the *joint* probability mass function of $X$ and $Y$. This is written as \n",
    "\n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "$$P\\{X=x,Y=y\\} = p(x,y)$$\n",
    "\n",
    "for discrete random variables, or\n",
    "\n",
    "$$P\\{X=x,Y=y\\} = f(x,y)dx dy$$\n",
    "\n",
    "for continuous ones.\n",
    "</div>\n",
    "\n",
    "Remember the capitalised $X$ is the random variable, $x$ is a value it can take on.\n",
    "\n",
    "How many values can this function take in the discrete case? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Short example\n",
    "\n",
    "Lets look at some data from last year, and set up two random variables:\n",
    "\n",
    "- $X=0$ if a student was doing straight maths, and $X=1$ if a student was doing any other course code.  \n",
    "- $Y=0$ if a student's surname did not begin with M, $Y=1$ if a students surname began with M.\n",
    "\n",
    "There are 4 joint probabilities - $X$ is 0 or 1 and $Y$ is 0 or 1. \n",
    "\n",
    "So we need to define $p(0,0), p(0,1),p(1,0),p(1,1)$\n",
    "\n",
    "These are the probabilities that both of the variables have specific values.\n",
    "\n",
    "| | $X=0$ | $X=1$ | sum |\n",
    "|-|-|-|-|\n",
    "|$Y=0$|$\\frac{22}{41}$|$\\frac{16}{41}$|$\\frac{38}{41}$|\n",
    "|$Y=1$|$\\frac{3}{41}$|$\\frac{0}{41}$|$\\frac{3}{41}$|\n",
    "|sum|$\\frac{25}{41}$|$\\frac{16}{41}$|$\\frac{41}{41}$|\n",
    "\n",
    "We found these values by looking through the spreadsheet of student information similar to the one we aggregated a few computational classes ago. \n",
    "\n",
    "The marginal values give us the individual probability mass functions of $X$, $p_X(x)$ and $Y$, $p_Y(y)$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Marginal distributions\n",
    "\n",
    "We can see this property of the marginal values (column or row sums) as each entry in the table is \n",
    "\n",
    "$$\n",
    "P\\{X=x,Y=y\\} = p(x,y) = P(X=x \\cap Y=y).\n",
    "$$\n",
    "\n",
    "We then have using the law of total probability\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap E_i) = \\sum_{i=1}^n P(A \\mid E_i) P(E_i)\n",
    "$$\n",
    "\n",
    "we can immediately see, specialising to our case where the $E_i$ are the values of $y$\n",
    "\n",
    "$$\n",
    "P\\{X=x\\} = \\sum_{\\text{all $y$}} P(X=x \\cap Y=y) = \\sum_{\\text{all $y$}}P(X=x \\mid Y=y) P(Y=y).\n",
    "$$\n",
    "\n",
    "and equally \n",
    "\n",
    "$$\n",
    "P\\{Y=y\\} = \\sum_{\\text{all $x$}} P(Y=y \\cap X=x) = \\sum_{\\text{all $x$}}P(Y=y \\mid X=x) P(X=x).\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Continuous random variables\n",
    "\n",
    "as normal, we replace the sums with integrals:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P\\{ x - \\Delta x /2 \\leq X \\leq x + \\Delta x /2\\}\\Delta x & = \\int_{-\\infty}^{\\infty} f(x,y) dy \\Delta x  \\\\\n",
    "                 & = f_X(x) \\Delta x \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and equally \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P\\{ y - \\Delta y /2 \\leq Y \\leq y + \\Delta y /2\\}\\Delta y & =\\int_{-\\infty}^{\\infty} f(x,y) dx \\Delta y \\\\\n",
    "                 & = f_Y(y) \\Delta y \n",
    "\\end{align}\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation of Sums of Random Variables\n",
    "\n",
    "We now come to a very important general result that will be the basis of a lot of the development we do later. \n",
    "\n",
    "What we will seek to show is that \n",
    "\n",
    "$$\n",
    "\\text{E}[X_1 + \\cdots + X_n] = \\text{E}[X_1] + \\cdots + \\text{E}[X_n],\n",
    "$$\n",
    "\n",
    "the expectation of the sum of the random variables $X_1 \\ldots X_n$ is just the sum of their individual expectations. \n",
    "\n",
    "Lets start with just two variables, $X$ and $Y$. We can use the general expression for the expectation of a function of two random variables\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[X+Y] & = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x+y)f(x,y) \\mathrm{d}x \\mathrm{d}y \\\\\n",
    "              & = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x f(x,y) \\mathrm{d}x \\mathrm{d}y + \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y f(x,y) \\mathrm{d}x \\mathrm{d}y \\\\\n",
    "              & = \\int_{-\\infty}^{\\infty} x f_{X}(x) \\mathrm{d}x + \\int_{-\\infty}^{\\infty} y f_{Y}(y) \\mathrm{d}y \\\\\n",
    "              & = \\text{E}[X] + \\text{E}[Y]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "At this point we can use proof by induction to show what we were after:\n",
    "\n",
    "$$\n",
    "\\text{E}[X_1 + \\cdots + X_n] = \\text{E}[X_1] + \\cdots + \\text{E}[X_n],\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**example**\n",
    "In fact we have already seen this result: \n",
    "\n",
    "We gave $X$ as the 'number of heads' when two coins were flipped.\n",
    "\n",
    "Let us define three new random variables, $X_1$ the number of heads on the first coin, $X_2$ the number of heads on the second coin, and then $X = X_1 + X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want the expectation value of $X_1 + X_2$. The expectation value of each is given by\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[X_1] & = \\text{E}[X_2] = \\sum_{\\text{all $x_1$}} x_1\\cdot p(x_1) \\\\\n",
    "              & = 0 \\cdot p(0) + 1 \\cdot p(1) \\\\\n",
    "              & = 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} \\\\ \n",
    "              & = \\frac{1}{2} = \\mu_{X_1} \\\\ \n",
    "              & = \\mu_{X_2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we can see that as expected\n",
    "\n",
    "$$\n",
    "\\text{E}[X] = \\text{E}[X_1] + \\text{E}[X_2] = 1\n",
    "$$\n",
    "\n",
    "what about the variance of the variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[X_1^2] & = \\text{E}[X_2^2] = \\sum_{\\text{all $x_1$}} x_1^2\\cdot p(x_1) \\\\\n",
    "& = 0 \\cdot p(0) + 1 \\cdot p(1)  \\\\\n",
    "& = 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2}  \\\\\n",
    "& = \\frac{1}{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\text{Var}(X_1) = \\text{Var}(X_2) = \\frac{1}{2} - (\\frac{1}{2})^2 =  \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "and we get **in this case** (because $X_1$ and $X_2$ are independent)\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\text{Var}(X_1) + \\text{Var}(X_2)\n",
    "$$\n",
    "\n",
    "It should be clear that this would hold for tossing three coins, four coins $\\ldots$ $n$ coins, which again could be proved by induction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation of Joint random variables.\n",
    "\n",
    "The joint probability mass/density function contains all the information about the variables it describes.\n",
    "\n",
    "We've just seen that we can retrieve the individual probability mass/density functions by integrating/summing over the other variables. \n",
    "\n",
    "If we have a general function of the variables, $g(X,Y)$, say, then we can obtain the expectation value of that function as\n",
    "\n",
    "$$\n",
    "\\text{E}[g(X,Y)] = \\sum_{y} \\sum_{x} g(x,y)p(x,y)\n",
    "$$\n",
    "\n",
    "for discrete random variables and \n",
    "\n",
    "$$\n",
    "\\text{E}[g(X,Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y)f(x,y) \\mathrm{d}x \\mathrm{d}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variances of Sums of Random variables.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "**if and only if the two variables are independent.**\n",
    "\n",
    "Or in general if $n$ variables are mututally independent. \n",
    "\n",
    "$$\n",
    "\\text{Var}(X_1 + X_2 + .. + X_n) = \\text{Var}(X_1) + \\text{Var}(X_2) + .. + \\text{Var}(X_n)\n",
    "$$\n",
    "\n",
    "This formula is actually a special case of something called the **covariance** between two random variables.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "the covariance between $X$ and $Y$, denoted by $\\text{Cov}(X,Y)$, is defined by\n",
    "$$\n",
    "\\text{Cov}(X,Y) = \\text{E}\\Big[ (X - \\text{E}[X])\\cdot(Y - \\text{E}[Y]) \\Big]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**example**\n",
    "back to our coins,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X_1,X_2) & = \\text{E}\\Big[ (X_1 - \\text{E}[X_1]) \\cdot (X_2 - \\text{E}[X_2]) \\Big] \\\\\n",
    "                    &=  \\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 - \\mu_{X_1} ) \\cdot (x_2 - \\mu_{X_2} )\\cdot p(x_1, x_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "were the second line comes from the general expression\n",
    "\n",
    "$$\n",
    "\\text{E}[g(X,Y)] = \\sum_{y} \\sum_{x} g(x,y)p(x,y)\n",
    "$$\n",
    "\n",
    "adjusted to our particular case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X_1,X_2) & = \\text{E} \\Big[ (X_1 - \\text{E}[X_1])\\cdot(X_2 - \\text{E}[X_2]) \\Big] \\\\\n",
    "                    &=  \\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 - \\mu_{X_1} ) \\cdot (x_2 - \\mu_{X_2} )\\cdot p(x_1, x_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case we can very reasonably expect that $p(x_1,x_2) = p(x_1)p(x_2)$ (remember the definition of independent probabilities). We can now write out all the terms in the double sum:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 - \\mu_{X_1} ) \\cdot (x_2 - \\mu_{X_2} )\\cdot p(x_1, x_2) = \\\\\n",
    "(0-\\frac{1}{2})\\times(0-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1-\\frac{1}{2})\\times(0-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(0-\\frac{1}{2})\\times(1-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1-\\frac{1}{2})\\times(1-\\frac{1}{2}) \\times \\frac{1}{2}\\times\\frac{1}{2} = \\\\\n",
    "\\frac{1}{16} - \\frac{1}{16} -\\frac{1}{16} + \\frac{1}{16} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "this will be the case whenever the two variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent variables have 0 covariance\n",
    "\n",
    "We can also see that the independence of $X$ and $Y$ guarantees that $\\text{Cov}(X,Y)=0$ from this expression by using the result that\n",
    "\n",
    "$$\n",
    "\\text{E}[XY] = \\text{E}[X]\\text{E}[Y]\n",
    "$$\n",
    "\n",
    "for independent variables. \n",
    "\n",
    "$$\n",
    "\\implies \\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y] = 0\n",
    "$$\n",
    "\n",
    "for independent variables. \n",
    "\n",
    "Again we can check that this holds for our $X_1$ and $X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful formula\n",
    "\n",
    "In a similar way to the variance the covariance can more easily be caluclated using an alternative expression\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,Y) & = \\text{E}\\Big[(X - \\text{E}[X])\\cdot(Y - \\text{E}[Y])\\Big] \\\\\n",
    "                & = \\text{E}\\Big[XY - \\text{E}[X]Y - X\\text{E}[Y] + \\text{E}[X]\\text{E}[Y]\\Big] \\\\\n",
    "                & = \\text{E}\\Big[XY] - \\text{E}[X]\\text{E}[Y] - \\text{E}[X]\\text{E}[Y] + \\text{E}[X]\\text{E}[Y]\\Big]\\\\ \n",
    "                & = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "calculate $\\text{E}[X_1 X_2]$ and $\\text{Cov}(X_1,X_2)$ for our two coin example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[X_1 X_2] = \\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 ) \\cdot (x_2 )\\cdot p(x_1, x_2) = \\\\\n",
    "\\sum_{\\text{all $x_1$}}\\sum_{\\text{all $x_2$}} (x_1 ) \\cdot (x_2 )\\cdot p(x_1) \\cdot p(x_2) = \\\\\n",
    "(0)\\times(0) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1)\\times(0) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(0)\\times(1) \\times \\frac{1}{2}\\times\\frac{1}{2} + \\\\\n",
    "(1)\\times(1) \\times \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "looking back we see that this is indeed equal to $\\text{E}[X_1]\\text{E}[X_2]$, so again\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X_1,X_1) & = \\text{E}\\Big[(X_1 - \\text{E}[X_1])\\cdot(X_2 - \\text{E}[X_2])\\Big] \\\\\n",
    "                    & = \\text{E}[X_1 X_2] - \\text{E}[X_1]\\text{E}[X_2] = 0\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance is a special case of covariance\n",
    "\n",
    "remember\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X,Y) = \\text{E} \\Big[(X - \\text{E}[X]) \\cdot (Y - \\text{E}[Y]) \\Big]\n",
    "$$\n",
    "\n",
    "in the special case that $X=Y$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}(X,X) & = \\text{E} \\Big[(X - \\text{E}[X])(X - \\text{E}[X]) \\Big] \\\\\n",
    "                & = \\text{E}\\Big[(X - \\text{E}[X])^2\\Big] \\\\\n",
    "                & = \\text{Var}(X)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so the covariance can have either sign, but the variance is the square of a value, so must be positive semi-definite ($\\geq 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "We have \n",
    "- defined the covariance of two random variables $X$ and $Y$\n",
    "- examined the meaning of the covariance\n",
    "- shown that $\\text{Cov}(X,Y) = 0$ for independent random variables."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
