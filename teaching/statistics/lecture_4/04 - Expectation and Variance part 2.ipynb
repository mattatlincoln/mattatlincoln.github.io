{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slides for Probability and Statistics module, 2015-2016\n",
    "# Matt Watkins, University of Lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary of relations given in part 1\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Given a random variable $X$ with a probability mass function $p(x)$  we define the expectation of $X$, written as $\\text{E}[X]$ as \n",
    "\n",
    "$$\n",
    "\\text{E}[X] = \\sum_{x \\in R_X} x\\cdot p(x)\n",
    "$$\n",
    "\n",
    "The expectation of a discrete random variable $X$ is just the arithmetic mean of the values it takes on.\n",
    "\n",
    "The equivalent for a continuous random variable $Z$ is\n",
    "\n",
    "$$\n",
    "\\text{E}[Z] =  \\int_{-\\infty}^{\\infty} z\\cdot f(z) \\mathrm{d}z\n",
    "$$\n",
    "\n",
    "again we see that the relationship is very close between discrete and continuous cases.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Let $g(X)$ be any function of a random variable $X$. Then\n",
    "\n",
    "$$\n",
    "\\text{E}[g(X)] = \\sum_{x \\in R_X}g(x) \\cdot p(x)\n",
    "$$\n",
    "\n",
    "or for the continuous random variable $Z$\n",
    "\n",
    "$$\n",
    "\\text{E}[g(Z)] = \\int_{-\\infty}^{\\infty} g(z)\\cdot f(z) \\mathrm{d}z\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "if $X$ is a random variable, then\n",
    "\n",
    "- $\\text{E}[a] = a$\n",
    "- $\\text{E}[aX] = a\\text{E}[X]$\n",
    "- $\\text{E}[g_1(X) + g_2(X)] = \\text{E}[g_1(X)] + \\text{E}[g_2(X)]$, where $g_1(X)$ and $g_2(X)$ are any functions of X. \n",
    "\n",
    "these define the properties of a linear operator. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance of a random variable\n",
    "\n",
    "The mean describes where a distribution is centred - expected value.\n",
    "\n",
    "The variance describes how widely the values of the distribution are spread around the mean.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: 8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "\n",
    "If $X$ is a random variable with mean $\\mu$, then the variance of $X$, denoted by $\\text{Var}(X)$ or $\\sigma_X^2$, is defined by\n",
    "\n",
    "$$\n",
    "\\sigma_X^2 = \\text{Var}(X) = \\text{E}[(X-\\mu)^2]\n",
    "$$\n",
    "\n",
    "this can also be written\n",
    "\n",
    "$$\n",
    "\\sigma_X^2 = \\text{Var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2 \n",
    "$$\n",
    "</div>\n",
    "\n",
    "It is the expected squared distance of a value from the centre of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard Deviation\n",
    "\n",
    "the standard deviation of a random variable $X$ is the square root of its variance.\n",
    "\n",
    "$$\n",
    "\\sigma_X = \\sqrt{\\text{Var}(X)} = \\sqrt{\\text{E}[X^2] - (\\text{E}[X])^2 } = \\sqrt{\\text{E}[(X-\\mu)^2]}\n",
    "$$\n",
    "\n",
    "a major advantage of the standard deviation is that it has the same units, if any, as the variable itself.\n",
    "\n",
    "It gives the expected root mean squared distance of a data point from the centre of the distribution.\n",
    "\n",
    "The smaller the variance or standard deviation, the more well defined a distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "show how the two formulae for the variance are equivalent\n",
    "\n",
    "$$\n",
    "\\sigma_X^2 = \\text{Var}(X) = \\text{E}[(X-\\mu)^2] =  \\text{E}[X^2] - (\\text{E}[X])^2 \n",
    "$$\n",
    "\n",
    "we can expand the bracket in $\\text{E}[(X-\\mu)^2]$ to get\n",
    "\n",
    "$$\n",
    "\\text{E}[(X-\\mu)^2] = \\text{E}[X^2 - 2\\mu X + \\mu^2]\n",
    "$$\n",
    "\n",
    "and we use the fact that it is a linear operator to write this as\n",
    "\n",
    "$$\n",
    "\\text{E}[X^2 - 2\\mu X + \\mu^2] = \\text{E}[X^2] - \\text{E}[2\\mu X] + \\text{E}[\\mu^2]\n",
    "$$\n",
    "\n",
    "and then that $\\text{E}[a] = a$ and $\\text{E}[aX] = a \\text{E}[X]$ to write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[X^2 - 2\\mu X + \\mu^2] & = \\text{E}[X^2] - \\text{E}[2\\mu X] + \\text{E}[\\mu^2] \\\\\n",
    "                               & = \\text{E}[X^2] - 2\\mu\\text{E}[X] + \\mu^2 \\\\\n",
    "                               & = \\text{E}[X^2] - 2\\mu \\mu + \\mu^2 \\\\\n",
    "                               & = \\text{E}[X^2] - (\\text{E}[X])^2 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "because by definition $\\mu = \\text{E}[X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful identity of the variance\n",
    "\n",
    "$$\n",
    "\\text{Var}(aX + b) = a^2 \\text{Var}(X)\n",
    "$$\n",
    "\n",
    "a shift of the distribution doesn't change its spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moments of a random variable\n",
    "\n",
    "The mean and variance describe a probability law for a random variable to some extent - the centre of the distribution $\\mu_X = \\text{E}[X]$ and how far points stray from the centre $\\sigma_X^2$.\n",
    "\n",
    "The $k^{th}$ moment of a random variable $X$ is defined as\n",
    "\n",
    "$m_k = \\text{E}[X^k]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "so \n",
    "$$\n",
    "m_1 = \\text{E}[X] = \\mu_X\n",
    "$$\n",
    "\n",
    "the second moment is\n",
    "\n",
    "$$\n",
    "m_2 = \\text{E}[X^2] = \\sigma_X^2 + \\mu_X^2\n",
    "$$\n",
    "\n",
    "so the mean and variance can be defined in terms of the first two moments of the distribution.\n",
    "\n",
    "Higher moments define skewedness ($m_3$) - how wonky a distribution is (zero for a symmetric distribution) - and other details of the shape of the distribution.\n",
    "\n",
    "A full set of moments fully defines a distribution (though sometimes not all moments will be well defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Moment generating function \n",
    "\n",
    "There are some short cuts to calculating the moments of a distribution\n",
    "\n",
    "The moment generating function of a random variable $X$ is\n",
    "\n",
    "$m_X(t) = \\text{E}[e^{tX}], -\\infty < t < \\infty$\n",
    "\n",
    "This function can be used to generate (hence the name) the moments of a distribution.\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$\n",
    "e^{tx} = 1 + tx + \\frac{(tx)^2}{2!} + \\frac{(tx)^3}{3!} + \\cdots\n",
    "$$\n",
    "\n",
    "the Taylor series of $e^{tx}$ about $x = 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_X(t) & = \\text{E}[e^{tX}] \\\\\n",
    "       & = \\text{E}\\Bigg[1 + tX + \\frac{(tX)^2}{2!} + \\frac{(tX)^3}{3!} + \\cdots \\Bigg]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and again using the linear properties of $\\text{E}[]$ we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_X(t) & =  1 + \\text{E}[tX] + \\frac{t^2}{2!}\\text{E}[X^2] + \\frac{t^3}{3!}\\text{E}[X^3] + \\cdots \\\\\n",
    "       & = 1 + t m_1 + \\frac{t^2}{2!} m_2 +  \\frac{t^3}{3!} m_3 + \\cdots\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we now take derivatives wrt (with respect to) $t$ we get\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}m_X(t)}{\\text{d}t}= m_1 +  t m_2 + \\frac{t^2}{2!} m_3 + \\cdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}^2m_X(t)}{\\text{d}t^2}= m_2 +  t m_3 + \\frac{t^2}{2!} m_4 + \\cdots\n",
    "$$\n",
    "\n",
    "finally setting $t = 0$ all terms in the series will be equal to zero except the first\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}m_X(t)}{\\text{d}t} \\Bigg|_{t=0} = m_1 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}^2m_X(t)}{\\text{d}t^2} \\Bigg|_{t=0} = m_2 \n",
    "$$\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example \n",
    "\n",
    "The length of time a transistor will work is a random variable $Y$ with density function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_Y(y) & = 0.001e^{-0.001y}, \\hfill y > 0 \\\\\n",
    "       & = 0 \\text{ otherwise:}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "the moment generating function of $Y$ is \n",
    "$$\n",
    "\\begin{align}\n",
    "m_Y(t) & = \\text{E}[e^{tY}]\\\\\n",
    "       & = \\int_0^{\\infty} e^{ty} 0.001e^{-0.001y} \\text{d}y\\\\\n",
    "       & = \\int_0^{\\infty} 0.001e^{-y(0.001-t)} \\text{d}y\\\\\n",
    "       & = \\frac{0.001}{0.001-t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "to find the moments we take the derivative with respect to $t$ then set $t = 0$\n",
    "\n",
    "$$\n",
    "\\mu = m^{(1)}(0) = 1000\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_2 = m^{(2)}(0) = 2(1000)^2\n",
    "$$\n",
    "\n",
    "so \n",
    "\n",
    "$$\n",
    "\\sigma^2 = m_2 - \\mu^2 = (1000)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "we have given the definitions of\n",
    "- expectation of random variables\n",
    "- variance of random variables\n",
    "- expectation of sums of random variables.\n",
    "\n",
    "next we will look at more examples and the expectations of joint random variables. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
