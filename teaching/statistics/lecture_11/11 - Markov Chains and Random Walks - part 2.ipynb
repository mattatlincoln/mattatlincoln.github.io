{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slides for Probability and Statistics module, 2016-2017\n",
    "# Matt Watkins, University of Lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Marokov Chain\n",
    "\n",
    "there are actually several types of Markov chain that can be categorized by the properties of their transition matrices. Their properties are qualitatively different in the long term.\n",
    "\n",
    "We'll only look at\n",
    "\n",
    "- Absorbing chains\n",
    "- Ergodic Chains\n",
    "    - Regular Chains\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Absorbing Markov Chains\n",
    "\n",
    "**definition**\n",
    "\n",
    "a Markov Chain is called absorbing if it has at least one state $s_i$ that is impossible to leave, $p_{ii} = 1$, and it is possible to reach an absorbing state from every state.\n",
    "\n",
    "**definition**\n",
    "\n",
    "states with $p_{ii} = 1$ are called absorbing states. Other states are called _transient states_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "A man walks along a four-block stretch of Park Avenue.  \n",
    "\n",
    "If he is at corner 1, 2, or 3, then he walks to the left or right with equal\n",
    "probability.\n",
    "\n",
    "He continues until he reaches corner 4, which is a bar, or corner 0, which is his home.  \n",
    "\n",
    "If he reaches either home or the bar, he stays there.\n",
    "\n",
    "![](PSfig11-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We form a Markov chain with states 0, 1, 2, 3, and 4.  States 0 and 4 are\n",
    "absorbing states.  \n",
    "\n",
    "The transition matrix is then\n",
    "\n",
    "$$\n",
    "\\textbf{P} =\\begin{matrix}\n",
    "  &  0  &  1  &  2  &  3  &  4  \\cr\n",
    "0 &  1  &  0  &  0  &  0  &  0  \\cr\n",
    "1 & 1/2 &  0  & 1/2 &  0  &  0  \\cr\n",
    "2 &  0  & 1/2 &  0  & 1/2 &  0  \\cr\n",
    "3 &  0  &  0  & 1/2 &  0  & 1/2 \\cr\n",
    "4 &  0  &  0  &  0  &  0  &  1 \\cr \\end{matrix}\n",
    "$$\n",
    "The states 1, 2, and 3 are transient states, and from any of these\n",
    "it is possible to reach the absorbing states 0 and 4.  \n",
    "\n",
    "The chain is an absorbing chain.  When a process reaches an absorbing state, we shall say that\n",
    "it is absorbed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Markov Chain diagrams\n",
    "\n",
    "it can be useful to depict a Markov Chain in diagramatic form. \n",
    "\n",
    "For instance for the Drunkard's walk we had\n",
    "\n",
    "![](PSfig11-3.png)\n",
    "\n",
    "the nodes show the states, $s_j$, whilst the arrows show the transition probabilities, $p_{ij}$. \n",
    "\n",
    "The diagrams make it easy to spot absorbing states, here the two end states, which have no arrows heading back to other states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visual picture\n",
    "\n",
    "you can make these diagrams prettier\n",
    "\n",
    "[visiualization of Markov Chains](http://setosa.io/ev/markov-chains/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**example**\n",
    "\n",
    "example of an absorbing Markov chain\n",
    "\n",
    "![](https://s-media-cache-ak0.pinimg.com/736x/03/c3/ab/03c3ab693eedcd84c1119a947974cb93.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The 100 is absorbing (in the sense the game ends at that point). The other tiles are transient states.\n",
    "\n",
    "It is quite possible to set up and solve this, finding the tiles that most time is spent on etc. \n",
    "\n",
    "[snakes and ladders analysed](http://www.datagenetics.com/blog/november12011/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Canonical Form\n",
    "\n",
    "If we rearrange the transition matrix for an absorbing chain so that the transient states are first, followed by the absorbing states we get the so called canonical form, which will have the general form\n",
    "\n",
    "![](absorb_I.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and it can be shown that\n",
    "\n",
    "![](absorb_II.png)\n",
    "\n",
    "it retains the same block structure when raised to the power $n$. We can extract the answers to the questions from the matrices $\\mathbf{Q}$ and $\\mathbf{R}$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the  probability that the process will eventually reach an absorbing state?\n",
    "- What is the probability that the process will end up in a given absorbing state?\n",
    "- On the average, how long will it take for the process to be absorbed?\n",
    "- On the average, how many times will the process be in each transient state?  \n",
    "\n",
    "The answers to all these questions depend, in general, on the\n",
    "state from which the process starts as well as the transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ergodic Markov Chains\n",
    "\n",
    "#### definition\n",
    "\n",
    "A Markov chain is called an ergodic chain if it is possible to go from every state to every state (though this can be in several moves).\n",
    "\n",
    "#### definition\n",
    "\n",
    "A Markov chain is called a regular chain if some power of the transition matrix has only positive elements.\n",
    "\n",
    "Every regular chain is ergodic, but not all ergodic chains are regular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long time behaviour\n",
    "\n",
    "Regular Markov chains have a well defined long term behaviour\n",
    "\n",
    "**theorem**\n",
    "Let $\\textbf{P}$ is the transition matrix from a regular chain. Then, as $n \\to \\infty$, the powers $\\textbf{P$^{(n)}$}$ approach a limiting matrix $\\textbf{W}$ with all rows the same vector $\\textbf{w}$.\n",
    "\n",
    "If we look at our Land of Oz example we can see this in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P^ 2 =\n",
      " [[ 0.4375  0.1875  0.375 ]\n",
      " [ 0.375   0.25    0.375 ]\n",
      " [ 0.375   0.1875  0.4375]] \n",
      "\n",
      "P^ 3 =\n",
      " [[ 0.40625   0.203125  0.390625]\n",
      " [ 0.40625   0.1875    0.40625 ]\n",
      " [ 0.390625  0.203125  0.40625 ]] \n",
      "\n",
      "P^ 4 =\n",
      " [[ 0.40234375  0.19921875  0.3984375 ]\n",
      " [ 0.3984375   0.203125    0.3984375 ]\n",
      " [ 0.3984375   0.19921875  0.40234375]] \n",
      "\n",
      "P^ 5 =\n",
      " [[ 0.40039062  0.20019531  0.39941406]\n",
      " [ 0.40039062  0.19921875  0.40039062]\n",
      " [ 0.39941406  0.20019531  0.40039062]] \n",
      "\n",
      "P^ 6 =\n",
      " [[ 0.40014648  0.19995117  0.39990234]\n",
      " [ 0.39990234  0.20019531  0.39990234]\n",
      " [ 0.39990234  0.19995117  0.40014648]] \n",
      "\n",
      "wP^n =  [ 0.4  0.2  0.4] \n",
      "\n",
      "wP =  [ 0.4  0.2  0.4] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = [0.4,0.2,0.4]\n",
    "\n",
    "P = np.array([[0.5, 0.25,0.25], [0.5, 0.0,0.5],[0.25,0.25,0.5]])\n",
    "Pn = [[0.4, 0.2,0.4], [0.4, 0.2,0.4],[0.4,0.2,0.4]]\n",
    "\n",
    "Pi = P\n",
    "\n",
    "for i in range (1,6):\n",
    "    Pi = np.dot(P,Pi)\n",
    "    print(\"P^\",i+1,\"=\\n\",Pi,\"\\n\")\n",
    "\n",
    "print(\"wP^n = \", np.dot(w,Pn),\"\\n\")\n",
    "print(\"wP = \",np.dot(w,P),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If this vector $\\textbf{w}$ is a steady state of the chain then we should have that $\\textbf{w} \\textbf{P} = \\textbf{w}\\lambda$.\n",
    "\n",
    "In fact we have that $\\textbf{w} \\textbf{P} = \\textbf{wI}$, where $\\textbf{I}$ is an identity matrix, because we need to preserve the amount of probability \n",
    "\n",
    "We can find $\\textbf{w}$ by linear algebra techniques:\n",
    "\n",
    "- find the eigenvalues/vectors of $\\textbf{P}$\n",
    "- directly solve $\\textbf{w} \\textbf{P} = \\textbf{wI}$, as we know that we want the eigenvector with eigenvalue 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Walks\n",
    "\n",
    "random walks can occur in spaces with various dimensions. The simplest ones work with a fixed step.\n",
    "\n",
    "We've seen one, in 1D - the difference between the number of heads and tails thrown when a coin is repeatedly tossed\n",
    "\n",
    "- at each step the walker can move up, with probability $p$, or down, with probability $1-p$ the number line\n",
    "\n",
    "In 2D (on a regular grid)\n",
    "\n",
    "- at each step the walker can move to one of the eight neighbouring sites\n",
    "\n",
    "In 3D (on a regular grid) we have something rather similar to diffusion\n",
    "\n",
    "- at each step the walker can move to one of the 9 + 8 + 9 = 26 neighbouring sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interestingly, there are qualitative differences between the walks in 1 and 2 or 3 dimensions.\n",
    "\n",
    "- It can be proved that in 1 or 2 D the walker will always return to its starting point.\n",
    "- In 3D the walker may never return (probability of return is approximately only 1/3)\n",
    "\n",
    "Grinstead and Snell sum this up as\n",
    "\n",
    ">\"One may summarize these results by stating that one should not get drunk in more than two dimensions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- describe the evolution between a series of states\n",
    "- Markov Chains depend on the current state, but not previous history\n",
    "- Initial state vector defines how the chain starts\n",
    "- Transition matrix (often called the stochastic matrix) built from  the transition probabilities between states\n",
    "- evolution occurs by applying the transition matrix repeatedly - each application is a step\n",
    "- use methods of linear algebra to explore long time properties of the chains"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
