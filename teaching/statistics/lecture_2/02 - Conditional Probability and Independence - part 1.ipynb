{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slides for Probability and Statistics module, 2016-2017\n",
    "# Matt Watkins, University of Lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional probability and independent\n",
    "\n",
    "Learning outcomes: \n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "<li> conditional probability</li>\n",
    "<li> definition of independence of events </li>\n",
    "<li> Bayes' Theorem</li>\n",
    "<li> Tree and Venn diagrams</li>\n",
    "<li> Law of total probability</li>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability function\n",
    "\n",
    "In the first lecture we defined the probability function (sometimes also called a distribution function) for a sample space, $S$. \n",
    "\n",
    "The sample space lists all possible outcomes of an experiment. For instance, for a single role of a 6-sided die, a possible sample space is $S = \\{1,2,3,4,5,6\\}$.\n",
    "\n",
    "It is a function that, if you feed it an outcome of an experiment, regurgitates a probability $x$,\n",
    "\n",
    "$p(i \\in S) = P(\\{i\\}) = 0 \\leq x \\leq 1$\n",
    "\n",
    "From this function we could calculate the probability of an event, $E$\n",
    "\n",
    "$P(E) = \\sum_{i \\in E} p(i)$\n",
    "\n",
    "I.e. we add together the probabilities of all the outcomes that are in the event, $E$.\n",
    "\n",
    "For instance, the 'getting an even number larger than 3 when rolling a single dice' = $A = \\{2,4,6\\} \\cap \\{4,5,6\\} =\\{4,6\\}$ is\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i \\in A} p(i) = p(4) + p(6) = 2/6\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional probability\n",
    "\n",
    "The idea of conditional probability allows us to calculate probabilities when we have partial information about the system, or to reassess likelihoods when we receive new information. \n",
    "\n",
    "A conditional probability is the probability of some event $E$, given that another event $F$ has occurred. \n",
    "\n",
    "The probability of $E$, given that $F$ has occurred is written \n",
    "\n",
    "$$\n",
    "P \\left(E \\mid F \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example** Consider rolling two dice. \n",
    "\n",
    "We have a sample space\n",
    "\n",
    "$$\n",
    "S = \\{(i,j), i=1,2,3,4,5,6, j=1,2,3,4,5,6\\}\n",
    "$$\n",
    "\n",
    "where say the outcome $(i,j)$ is the first die lands with $i$ dots up and the second die with $j$. \n",
    "\n",
    "We assume the dice are fair - we assign a probability (distribution) function of $P(\\{(i,j)\\}) = p(i,j) = \\frac{1}{36}$ to all outcomes.\n",
    "\n",
    "Suppose the first die comes down a three, what is the probability that the sum of the two dice equals eight?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define $F = \\text{'the first die comes down showing a 3'} =\\{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6)\\}$. \n",
    "\n",
    "This extra information, allows us to cut down the number of possibile outcomes. Effectively only 6 possible outcomes can now occur. \n",
    "\n",
    "Each of these remaining outcomes is still equally likely (what do you think?). \n",
    "\n",
    "So to get the probability we are after we need to find the proportion of the remaining 6 outcomes that are in\n",
    "\n",
    "$E = \\text{'the sum of the dice = 8'} = \\{(2,6),(3,5),(4,2),(5,3),(6,2)\\}$.\n",
    "\n",
    "The only outcome that satisfies the criteria is $\\{(3,5)\\}$ and once we know that $F$ has occurred, it has a one in 6 chance that $\\{(3,5)\\}$ will occur.\n",
    "\n",
    "$$\n",
    "P(E \\mid F ) = \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "Note also that $\\{(3,5)\\} = E \\cap F$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The probability of the outcome (3,5) before the first die was rolled was $\\frac{1}{36}$. After the first die comes up a 3, we've argued that the probability should be $\\frac{1}{6}$. Lets look at this as a Venn diagram:\n",
    "\n",
    "![](../Images/conditional_prob_venn_I.jpg)\n",
    "\n",
    "where $S$ is the sample space, and we have labelled two events $E$ and $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now the probability of $E$ occuring is the probability of the outcomes in $E$.\n",
    "\n",
    "![](../Images/conditional_prob_venn_II.jpg)\n",
    "\n",
    "If we are dealing with finite sets with equal likelihood, this probability is \n",
    "\n",
    "$$\\frac{n(E)}{n(S)}$$.\n",
    "\n",
    "where $n(E)$ is the number of ways that the event $E$ can occur, and $n(S)$ the same for $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If instead we are looking for $P(E \\mid F)$ then it is not the ratio $n(E)$ to $n(S)$ that matters\n",
    "\n",
    "![](../Images/conditional_prob_venn_III.jpg)\n",
    "\n",
    "we know that event $F$ has occured - so now we want the number of ways that both $E$ and $F$ can occur, but now our effective sample space is just $F$. This gives\n",
    "\n",
    "$$\n",
    "P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}\n",
    "$$\n",
    "\n",
    "For discrete equal likelihood sample spaces this is:\n",
    "\n",
    "$$\n",
    "P(E \\mid F) = \\frac{n(E \\cap F)}{n(S)}\\frac{n(S)}{n(F)}= \\frac{n(E \\cap F)}{n(F)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we look back to our dice example, this is exactly what we did.\n",
    "\n",
    "**Example revisited** we started from \n",
    "\n",
    "$$\n",
    "S = \\{(i,j), i=1,2,3,4,5,6, j=1,2,3,4,5,6\\},\n",
    "$$\n",
    "\n",
    "and asked, \n",
    ">\"Suppose the first die comes down with a three, what is the probability that the sum of the two dice equals eight\"? \n",
    "\n",
    "In the language we've just used, our event $E$ is $\\{(2,6),(6,2),(3,5),(5,3),(4,4)\\}$, the event $F = \\{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6)\\}$. \n",
    "\n",
    "So $E \\cap F = \\{(3,5)\\}$.\n",
    "\n",
    "Loking at the size of these events (all individual outcomes are equally likely), we have\n",
    "\n",
    "$$\n",
    "P(E \\mid F) = \\frac{n(E \\cap F)}{n(S)}\\frac{n(S)}{n(F)}= \\frac{n(E \\cap F)}{n(F)} = \\frac{1}{6}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "**Definition**\n",
    "the conditional probability of $E$ given $F$ is \n",
    "\n",
    "$$\n",
    "P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "combined with the standard properties of probabilities this implies\n",
    "\n",
    "- $P(\\bar{E} \\mid F) = 1 - \\frac{P(E \\cap F)}{P(F)}$\n",
    "- if $E$ and $F$ are mutually exclusive, then $P(E \\mid F) = 0$ because $E \\cap F = \\emptyset$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiplicative rule of probabilities\n",
    "\n",
    "the expression for conditional probabilities implies that\n",
    "\n",
    "$$\n",
    "P(E \\cap F) = P(E \\mid F)P(F).\n",
    "$$\n",
    "\n",
    "the probability that $E$ and $F$ occur is the probability that $F$ occurs multiplied by the probability that $E$ occurs given that $F$ has occurred.\n",
    "\n",
    "This can be generalised to the case of many events $E_i$\n",
    "\n",
    "$$\n",
    "P(E_1 \\cap E_2 \\cap E_3 \\cap \\dots \\cap E_n)= P(E_1) \\cdot P(E_2 \\mid E_1) \\cdot P(E_3 \\mid E_1 \\cap E_2) \\cdots P(E_n \\mid E_1 \\cap \\cdots \\cap E_{n-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "we can see that this is the case by using the definition of conditional probability in the right hand side of the equation, giving\n",
    "\n",
    "$$\n",
    "P(E_1) \\frac{P(E_1 \\cap E_2)}{P(E_1)} \\cdot \\frac{P(E_1 \\cap E_2 \\cap E_3)}{P(E_1 \\cap E_2)} \\cdots \\frac{P(E_1 \\cap E_2 \\cdots \\cap E_n)}{P(E_1 \\cap E_2 \\cdots \\cap E_{n-1})}\n",
    "$$\n",
    "\n",
    "each internal term of the numerator cancels with the next term in the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "If you remember the examples of taking  balls out of a bowl in lecture 1 - there were 6 black and 5 white ones.\n",
    "\n",
    "How many ways are there to select all the balls from the bowl?\n",
    "\n",
    "In this case there are $11⋅10⋅9 \\cdots 3⋅2⋅1  = 11!$\n",
    "\n",
    "We could calculate this as follows: Each permutation is equally likely, so we can calculate the probability of getting a particular selection, which would be $1/N$, so we find $N$ by the inverse of the probability.\n",
    "\n",
    "If we look for the particular selection $\\{(1,2,3,4,5,6,7,8,9,10,11)\\}$, where we've numbered the balls, then\n",
    "\n",
    "We define $E_n = '\\text{the nth ball is ball n}'$\n",
    "\n",
    "$$\n",
    "P(E_1 \\cap E_2 \\cap E_3 \\cap \\dots \\cap E_n) = P(E_1) \\cdot P(E_2 \\mid E_1) \\cdot P(E_3 \\mid E_1 \\cap E_2) \\cdots P(E_n \\mid E_1 \\cap \\cdots \\cap E_{n-1})\n",
    "$$\n",
    "\n",
    "And we'd get $P(E_1 \\cap E_2 \\cap E_3 \\cap \\dots \\cap E_n) = 1 / 11 \\cdot 1/10 \\cdot 1/9 \\cdots 1/3 \\cdot 1/2 \\cdot 1/1) = 1/11! = |E_1 \\cap E_2 \\cap E_3 \\cap \\dots \\cap E_n|/N = 1/N \\implies N = 11!$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree diagrams\n",
    "\n",
    "Tree diagrams are a useful way of keeping track of the progress of compound experiments. \n",
    "\n",
    "They can also be seen to work using the multiplicative rule of probabilities.\n",
    "\n",
    "Let's analyse throwing 3 coins sequentially.\n",
    "\n",
    "![](../Images/tree_3_coins.png)\n",
    "\n",
    "The overall probabilities are arrived at by multiplying the probabilities at each stage of the experiment. This works because at each stage in the compound experiment, the result doesn't depend on the result further down the chain. So to get \n",
    "\n",
    "$P(\\omega_1) = P(\\{(H,H,H)\\}) = P(\\{H\\}) \\cdot P(\\{(H,H)\\} \\mid \\{H\\}) \\cdot P(\\{(H,H,H)\\} \\mid \\{(H,H)\\}) = P(\\{\\{H\\}\\}) \\cdot P(\\{\\{H\\}\\}) \\cdot P(\\{\\{H\\}\\}) = p^3$\n",
    "\n",
    "because each probability $P(\\{(H,H,H)\\} \\mid \\{(H,H)\\})$ only depends on the current coin flip, not anything else. \n",
    "\n",
    "In other words the individual parts of the compound experiment are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "We have two urns, I and II. \n",
    "\n",
    "Urn I contains 2 black balls and 3 white balls. \n",
    "\n",
    "Urn II contains 1 black ball and 1 white ball. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An urn is selected at random and a ball is chosen at random from it. \n",
    "\n",
    "We can represent the sample space of this experiment as the paths through a tree as shown. \n",
    "\n",
    "![](../Images/tree_urns.png)\n",
    "\n",
    "The probabilities assigned to the paths are also shown. \n",
    "\n",
    "Let $B = \\text{“a black ball is drawn,”}$ and $I = \\text{“urn I is chosen.”}$. \n",
    "\n",
    "Then the branch weight $\\frac{2}{5}$, which is shown on one branch in the figure, can now be interpreted as the conditional probability $P(B \\mid I)$. \n",
    "\n",
    "If there were more branchings, we would have to use the multiplicative rule of probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example - back to the Urns and balls\n",
    "\n",
    "Suppose that we wanted to work out $P(I \\mid B)$? \n",
    "\n",
    "In words \"What is the probability that the ball came from urn I, given that the ball is black.\"\n",
    "\n",
    "We can do this as follows\n",
    "\n",
    "$$\n",
    "P(I \\mid B) = \\frac{P(I \\cap B)}{P(B)} = \\frac{P(I \\cap B)}{P(B \\cap I ) + P(B \\cap II)} = \\frac{\\frac{1}{5}}{\\frac{1}{5}+\\frac{1}{4}} = \\frac{4}{9}\n",
    "$$\n",
    "\n",
    "We can repeat this for other conditional probabilities and use it to construct a reverse tree diagram.\n",
    "\n",
    "![](../Images/tree_urns_reverse.png)\n",
    "\n",
    "These 'reversed' conditional probabilities are often associated with the name Bayes. \n",
    "\n",
    "We'll look at a formula that bears his name shortly that can be used to systematically calculate these reversed probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Law of total probability \n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "We can split $S$ up into an exhaustive collection of mutually exclusive events $E_i$ such that\n",
    "\n",
    "$$\n",
    "\\bigcup_{i=1}^n E_i = S\n",
    "$$\n",
    "\n",
    "remembering that mutually exclusive means that $E_ i \\cap E_j = \\emptyset, i \\neq j$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "In Venn diagram form this collection of events could look like this\n",
    "\n",
    "![](../Images/Bayes_proof_I.png)\n",
    "\n",
    "Where $A \\subset S$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can then use the $E_i$ (a bit like basis vectors in linear algebra) to decompose an event $A$ into its intersections with the $E_i$. \n",
    "\n",
    "$$\n",
    "A =\\bigcup_{i=1}^n A \\cap E_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../Images/Bayes_proof_II.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "The law of total probability\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap E_i) = \\sum_{i=1}^n P(A \\mid E_i) P(E_i)\n",
    "$$\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is the general form of the [law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability) (the name is not so firmly established that it will be called this by all authors, though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Formula\n",
    "<br>\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "$$\n",
    "P(E_i \\mid A) = \\frac{P(A \\cap E_i)}{P(A)} = \\frac{P(A \\mid E_i) P(E_i)}{\\sum_{i=j}^n P(A \\mid E_j) P(E_j)}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Where we have used the law of total probability, $P(A) = \\sum_{i=j}^n P(A \\mid E_j) P(E_j)$, to expand the denominator.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Thomas_Bayes.gif/225px-Thomas_Bayes.gif)\n",
    "\n",
    "no known picture of Bayes exists, apparently. But this didn't stop people illustrating him in a 1936 book! This is probably just some bloke. \n",
    "\n",
    "Bayes' solution to a problem of inverse probability was presented in \"An Essay towards solving a Problem in the Doctrine of Chances\" which was read to the Royal Society in 1763 after Bayes' death."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we look at the formula, we see that the two types of conditional probabilities have swapped events on the left and right hand sides.\n",
    "\n",
    "$$\n",
    "P(E_i \\mid A) = \\frac{P(A \\mid E_i) P(E_i)}{P(A)}\n",
    "$$\n",
    "\n",
    "Bayes' Formula can be looked at as a way updating our opinion on a problem when new data arrives: \n",
    "\n",
    "in this point of view the $E_i$ represent our prior opinion about the likelihood of events - the probabilities associated with these hypotheses are then updated when we find out that $A$ has occurred.\n",
    "\n",
    "It is possible to use this sort of process for data fitting.\n",
    "\n",
    "Bayesian is the other main 'ideology' in probability theory / interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "A town has three bus routes, A,B and C. \n",
    "\n",
    "During rush hour there are twice as many buses on the A route as on B or C. \n",
    "\n",
    "Over a period of time it has been observed that at a crossroads, where the routes converge, the buses run more than 5 minutes late $\\frac{1}{2}, \\frac{1}{5},\\frac{1}{10}$ of the time.\n",
    "\n",
    "If an inspector at the crossroads finds that the first bus he sees is more than five minutes late, what it the chance that it is a route B bus?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We don't have complete information about this problem (like all possible outcomes etc). \n",
    "\n",
    "We can form an exhaustive partition of the experiment into the mutually exhaustive events $A,B,C$ that the inspector sees a bus on that route first. \n",
    "\n",
    "And we have the event $L$ that the bus is late. \n",
    "\n",
    "We see that the problem looks like our Bayes' Formula situation: we know the probabilities $P(L \\mid A), P(L \\mid B), P(L \\mid C)$, but we want to know $P(B \\mid L)$.\n",
    "\n",
    "![](../Images/Bayes_bus_prob.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We require $P(B \\mid L)$. Using Bayes' Formula we get that\n",
    "\n",
    "$$\n",
    "P(B \\mid L) = \\frac{P(B)\\cdot P(L \\mid B)}{P(A)\\cdot P(L \\mid A) + P(B)\\cdot P(L \\mid B) + P(C)\\cdot P(L \\mid C)}\n",
    "$$\n",
    "\n",
    "From the information we have we can work out that $P(A)=\\frac{1}{2}$ and $P(B) = P(C) = \\frac{1}{4}$. \n",
    "\n",
    "Also we are given that $P(L \\mid A) = \\frac{1}{2}, P(L \\mid B) = \\frac{1}{5}, P(L \\mid C) = \\frac{1}{10}$\n",
    "\n",
    "Plugging these numbers in gives $P(B \\mid L) = \\frac{2}{13}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probabilities are probabilities\n",
    "\n",
    "It is good to note that conditional probabilities obey all the axioms we established to call $P(E)$ a probability. \n",
    "\n",
    "For the statement above to be true our conditional probabilities must satisfy the axioms of probabilities:\n",
    "\n",
    "- $0 \\leq P(E \\mid F) \\leq 1$\n",
    "- $P(S) = 1$\n",
    "- if $E_i, i =1 \\ldots n $ are mutually exclusive events then$$P \\left( \\bigcup_{i-1}^n E_i \\mid F \\right) = \\sum_{i=1}^n P(E_i \\mid F) $$\n",
    "\n",
    "I won't prove these results here, but they are in textbooks and online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of conditional probabilities\n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "- Definition of conditional probabilities $$\n",
    "P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}\n",
    "$$\n",
    "- Multiplicative rule $$\n",
    "P(E_1 E_2 E_3 \\dots E_n) = P(E_1) \\frac{P(E_1 \\cap E_2)}{P(E_1)} \\frac{P(E_1 \\cap E_2 \\cap E_3)}{P(E_1 \\cap P(E_2)} \\cdots \\frac{P(E_1 \\cap E_2 \\cdots \\cap E_n)}{P(E_1 \\cap E_2 \\cdots \\cap E_{n-1})}\n",
    "$$\n",
    "- Law of total probability $$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap E_i) = \\sum_{i=1}^n P(A \\mid E_i) P(E_i)\n",
    "$$\n",
    "\n",
    "- Bayes' Formula $$\n",
    "P(E_i \\mid A) = \\frac{P(A \\cap E_i)}{P(A)} = \\frac{P(A \\mid E_i) P(E_i)}{\\sum_{j=1}^n P(A \\mid E_j) P(E_j)}\n",
    "$$\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
