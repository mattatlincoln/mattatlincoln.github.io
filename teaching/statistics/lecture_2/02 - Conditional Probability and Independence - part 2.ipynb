{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slides for Probability and Statistics module, 2016-2017\n",
    "# Matt Watkins, University of Lincoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of conditional probabilities\n",
    "\n",
    "- Definition of conditional probabilities $$\n",
    "P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}\n",
    "$$\n",
    "- Multiplicative rule $$\n",
    "P(E_1 E_2 E_3 \\dots E_n) = P(E_1) \\frac{P(E_1 \\cap E_2)}{P(E_1)} \\frac{P(E_1 \\cap E_2 \\cap E_3)}{P(E_1 \\cap P(E_2)} \\cdots \\frac{P(E_1 \\cap E_2 \\cdots \\cap E_n)}{P(E_1 \\cap E_2 \\cdots \\cap E_{n-1})}\n",
    "$$\n",
    "- Law of total probability $$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap E_i) = \\sum_{i=1}^n P(A \\mid E_i) P(E_i)\n",
    "$$\n",
    "\n",
    "- Bayes' Formula $$\n",
    "P(E_i \\mid A) = \\frac{P(A \\cap E_i)}{P(A)} = \\frac{P(A \\mid E_i) P(E_i)}{\\sum_{j=1}^n P(A \\mid E_j) P(E_j)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Independent events\n",
    "\n",
    "\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "$\\textbf{Definition}$ Two experiments are *independent* if the result of one can not in any way affect the possible results of the other.<br><br>\n",
    "\n",
    "$\\textbf{Definition}$ Two events ($E, F$) are *independent* if the probability that one of them occurs is in no way influenced by whether or not the other has occurred. \n",
    "<br>\n",
    "So  \n",
    "\n",
    "\\begin{align}\n",
    "P(E) = P(E \\mid F) = P(E \\mid \\bar{F}), \\\\\n",
    "P(F) = P(F \\mid E) = P(F \\mid \\bar{E}).\n",
    "\\end{align}\n",
    "\n",
    "put in a different way this means that\n",
    "\n",
    "$$\n",
    "P(E \\cap F) = P(E)P(F)\n",
    "$$\n",
    "\n",
    "the probability of $E$ and $F$ occurring is just the product of the probability of $E$ occuring and the probability of $F$ occurring.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../Images/probmean3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "**Theorem** Two events ($E, F$) are *independent* if and only if\n",
    "\n",
    "$$\n",
    "P(E \\cap F) = P(E)P(F)\n",
    "$$\n",
    "</div><br>\n",
    "For more than two events things become a bit more restrictive.\n",
    "<br><br>\n",
    "<div style=\"background-color:Gold; margin-left: 20px; margin-right: 20px; padding-bottom: 8px; padding-left: \n",
    "8px; padding-right: 8px; padding-top: 8px; border-radius: 25px;\">\n",
    "\n",
    "**Theorem** The events $E_1, E_2, E_3, \\cdots E_n$ are said to be mutually independent if for every subset $E_1', E_2', E_3', \\cdots E_r', r \\leq n$ of the events\n",
    "\n",
    "$$\n",
    "P(E_1'\\cap E_2' \\cap E_3' \\cap \\cdots \\cap E_r') = P(E_1') \\cdot P(E_2') \\cdot P(E_3') \\cdots P( E_r')\n",
    "$$\n",
    "</div>\n",
    "\n",
    "The idea of independent processes will be extremely important as we move forward. \n",
    "\n",
    "We will use the idea that repetitions of experiments constitute independent processes very often. Note that this is more or less an assumption of the frequentist definition of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "consider the compound experiment of throwing two fair coins.\n",
    "\n",
    "The sample space is $S = \\{(H,H),(H,T),(T,H),(T,T)\\}$.\n",
    "\n",
    "Define two events \n",
    "\n",
    "$A = \\textrm{'the first coin is a head'} = \\{(H,H),(H,T)\\}$\n",
    "\n",
    "$B = \\textrm{'the second coin is a tail'} = \\{(H,T),(T,T)\\}$\n",
    "\n",
    "And $P(A) = |A|/|S| = 2 /4  = 1/2$ and $P(B) = |B|/|S| = 2/4 = 1/2$.\n",
    "\n",
    "Now $P(A \\cap B) = P({H,T}) = |A \\cap B|/|S| = 1/4 = P(A) * P(B)$ so $A$ and $B$ are independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But consider\n",
    "\n",
    "$C = \\textrm{'both coins are heads'} = \\{(H,H)\\}$\n",
    "\n",
    "$P(C) = |C| / |S| = 1/4$\n",
    "\n",
    "now $P(B \\cap C) = P(\\emptyset) = |B \\cap C|/|S| = 0/4 \\neq P(A) * P(B)$, so $B$ and $C$ are not independent.\n",
    "\n",
    "$A$ is also not independent of $C$.\n",
    "\n",
    "In general, it is possible for all pairs of events to be independent, but the complete set of events not to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tabular presentation of conditional probabilities\n",
    "\n",
    "It can sometimes be handy to view conditional probabilities using a tabular representation of relative frequencies - this can also be how real data arrives to us.\n",
    "\n",
    "**Example** If we go back to the bus problem we did before the break, but instead we consider what we'd expect if 1000 buses in total ran through the town, we'd end up with something like\n",
    "\n",
    "| |A |B |C |total |\n",
    "|-|-|-|-|-|\n",
    "|Late |250|50|25|325|\n",
    "|Not late |250|200|225|675|\n",
    "|Total |500|250|250|1000|\n",
    "\n",
    "Let $L = \\textrm{'a bus is late'}$\n",
    "\n",
    "The conditional probabilities can easily be read off, for instance $P(B \\mid L) = P(B \\cap L)/ P(L) = \\frac{50}{325} = \\frac{2}{13}$. \n",
    "\n",
    "This is of course exactly equivalent to our pen and paper solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values along the margins are called the marginal probabilities. They give the straight forward probabilities. \n",
    "\n",
    "This is because of the law of total probability.\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap E_i) = \\sum_{i=1}^n P(A \\mid E_i) P(E_i)\n",
    "$$\n",
    "\n",
    "In this case \n",
    "\n",
    "$$P(L) = P(L \\cap A) + P(L \\cap B) + P(L \\cap C) = 250 + 50 +25 = 325$$\n",
    "\n",
    "or vertically we have\n",
    "\n",
    "$$P(A) = P(A \\cap L) + P(A \\cap \\bar{L})$$\n",
    "\n",
    "note that that last relationship is a useful and general one. It is a special case of the law of total probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tabular Bayes' Theorem example\n",
    "\n",
    "A doctor is trying to decide if a patient has one of three diseases d1, d2, or d3. \n",
    "\n",
    "Two tests are to be carried out, each of which results in a positive (+) or a negative (−) outcome. \n",
    "\n",
    "There are four possible test patterns ++, +−, −+, and −−. \n",
    "\n",
    "National records have indicated that, for 10,000 people having one of these three diseases, the distribution of diseases and test results are as in the table below.\n",
    "\n",
    "|Disease|number| + +| + –| – +| – –|\n",
    "|-|-|-|-|-|-|\n",
    "|d1 |3215| 2110| 301| 704| 100|\n",
    "|d2 |2125| 396| 132| 1187| 410|\n",
    "|d3 |4660| 510| 3568 |73| 509|\n",
    "|Total |10000||||||\n",
    "\n",
    "We can use this data to estimate $P(d_1),P(d_2),P(d_3)$ - these are called prior probabilities, and the conditional probabilities like $P(+- \\mid d_1) = \\frac{301}{3215}=0.094$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "What the doctor wants though is the probability a patient has disease $d_i$ given the results of the tests. These are the Bayes' or inverse, or posterior probabilities. \n",
    "\n",
    "We can compute them using Bayes' formula and we'll get results like\n",
    "\n",
    "||$d_1$|$d_2$|$d_3$|\n",
    "|-|-|-|-|\n",
    "|+ +| .700| .131| .169|\n",
    "|+ –| .075| .033| .892|\n",
    "|– +| .358| .604| .038|\n",
    "|– –| .098| .403| .499|\n",
    "\n",
    "these are $P(d_i \\mid ++)$ etc. Judicious use of these posterior probabilities can be used to inform decision making: \n",
    "\n",
    "In this case the prior probability of a patient having disease $d_1$ was $\\frac{3215}{10000} = 0.3215$. \n",
    "\n",
    "If the test result came back ++ then the posterior probability $P(d_1 \\mid ++) = 0.700$ and we'd suspect that $d_1$ was the culprit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Updating information sequentially\n",
    "\n",
    "In the last example we updated the likelihood of a patient having a particular disease based on extra information in the form of the test results.\n",
    "\n",
    "The previous example with the medical data should give a hint how we can update our ideas about a system as new information comes in - this is why the original probabilities are call priors, and the reversed conditional probabilities are called posterior probabilities. \n",
    "\n",
    "We'll use this type of method in the computing lab later as a simple form of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous Conditional Probability\n",
    "\n",
    "we've stayed focussed upon discrete probability distributions. \n",
    "\n",
    "However similar observations also apply to the case of continuous probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continous probability spaces\n",
    "\n",
    "Consider a spinner - schematically a circle _of unit circumference_ and a pointer\n",
    "\n",
    "![](../Images/spinner.jpg)\n",
    "\n",
    "this could end up being a model for a [Roulette wheel](https://en.wikipedia.org/wiki/Roulette), for instance. \n",
    "\n",
    "If we give the spinner a whirl, the pointer will be pointing somewhere a distance $x$ along the circumference. \n",
    "\n",
    "It seems reasonable that every value $0 \\leq x \\lt 1$ of the distance between the pointer and the mark on the spinner is equally likely to occur. This means that the sample space is the interval  $S = [0,1)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can satisfy\n",
    "\n",
    "$$\n",
    "P\\left( a\\leq X \\lt b \\right) = b - a\n",
    "$$\n",
    "\n",
    "for every $a$ and $b$ for the event $E = [a,b]$ by a formula of the form\n",
    "\n",
    "$$\n",
    "P(E) = \\int_{E} f(x) \\mathrm{d}x,\n",
    "$$\n",
    "\n",
    "and $f(x)$ is the constant function with value 1. \n",
    "\n",
    "We call $f(x)$ the _density function_ of $X$. \n",
    "\n",
    "This is the generalisation of the discrete case we saw earlier:\n",
    "\n",
    "$$\n",
    "P(E) = \\sum_{i \\in E} P(i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want a probability model where every value of the sample space is equally likely (we'll call the result of a spin $X$ for now, later we'll see that this is a _continuous random variable_).\n",
    "\n",
    "In a similar way to the discrete case we must have\n",
    "\n",
    "$$\n",
    "P\\left( 0\\leq X \\lt 1 \\right) = 1.\n",
    "$$\n",
    "\n",
    "It is also the case that we expect the probability of a reading in the top half of the spinner is equal in likelihood to one in the lower half,\n",
    "\n",
    "$$\n",
    "P\\left( 0\\leq X \\lt \\frac{1}{2} \\right) = P\\left( \\frac{1}{2} \\leq X \\lt 1 \\right) = \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    "More generally, if we consider an event, $E = \\{[a,b]\\} $, we'd like\n",
    "\n",
    "$$\n",
    "P\\left( a\\leq X \\lt b \\right) = b - a\n",
    "$$\n",
    "\n",
    "\n",
    "for every $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional continuous probabilities\n",
    "\n",
    "If we look at a process that has a density function $f(x)$, and if $E$ is an event. We define a conditional density function by\n",
    "\n",
    "$$\n",
    "f(x \\mid E) = \\Bigg \\{ \\begin{array}{ll}\n",
    "f(x)/P(E) & \\mbox{if  $x \\in E$},\\\\\n",
    "0 & \\mbox{if  $x \\notin E$}\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "Then for any event $F$, we have\n",
    "\n",
    "$$\n",
    "P(F \\mid E) = \\int_F f(x \\mid E)\\ \\mathrm{d}x.\n",
    "$$\n",
    "\n",
    "We call this the conditional probability of $F$ given $E$. A little manipulation makes the connection to the discrete case:\n",
    "\n",
    "$$\n",
    "P(F \\mid E) = \\int_F f(x \\mid E)\\ \\mathrm{d}x = \\int_{E \\cap F} \\frac{f(x)}{P(E)}\\mathrm{d}x = \\frac{P(E \\cap F)}{P(E)}$$\n",
    "\n",
    "Definition of conditional probabilities $$\n",
    "P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of conditional continuous probability distribution\n",
    "\n",
    "In the spinner experiment, suppose we know that the spinner has stopped with head in the upper half of the circle, $0 \\leq x \\leq 1/2$. What is the probability that $1/6 \\leq x \\leq 1/3$? \n",
    "\n",
    "Here $E = \\{[0, 1/2]\\}, F = \\{[1/6, 1/3]\\}$\n",
    "\n",
    "Also we note that $F \\cap E = F$. \n",
    "\n",
    "Hence$$\n",
    "P(F \\mid E) = P(F \\cap E)P(E)=\\frac{\\frac{1}{6}}{\\frac{1}{2}}=\\frac{1}{3}\n",
    "$$, which is reasonable, since $F$ is $1/3$ the size of $E$. \n",
    "\n",
    "The conditional density function here is given by \n",
    "$$f(x \\mid E) = \\Bigg \\{ \\begin{array}{ll}\n",
    "2, & \\mbox{if  0 ≤ x < 1/2},\\\\\n",
    "0, & \\mbox{if 1/2 ≤ x < 1}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus the conditional density function is nonzero only on $[0, 1/2]$, and is uniform there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Definition of conditional probabilities $$\n",
    "P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}\n",
    "$$\n",
    "- Law of total probability $$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap E_i) = \\sum_{i=1}^n P(A \\mid E_i) P(E_i)\n",
    "$$\n",
    "\n",
    "- Bayes' Formula $$\n",
    "P(E_i \\mid A) = \\frac{P(A \\cap E_i)}{P(A)} = \\frac{P(A \\mid E_i) P(E_i)}{\\sum_{j=1}^n P(A \\mid E_j) P(E_j)}\n",
    "$$\n",
    "\n",
    "- Two events ($E, F$) are *independent* if and only if\n",
    "\n",
    "$$\n",
    "P(E \\cap F) = P(E)P(F)\n",
    "$$\n",
    "\n",
    "- The events $E_1, E_2, E_3, \\cdots E_n$ are said to be mutually independent if for every subset $E_1', E_2', E_3', \\cdots E_r', r \\leq n$ of the events\n",
    "\n",
    "$$\n",
    "P(E_1'\\cap E_2' \\cap E_3' \\cap \\cdots \\cap E_r') = P(E_1') \\cdot P(E_2') \\cdot P(E_3') \\cdots P( E_r')\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
